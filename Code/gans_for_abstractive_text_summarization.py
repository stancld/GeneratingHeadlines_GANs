# -*- coding: utf-8 -*-
"""GANs for Abstractive Text Summarization.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MBpliNJ1d0gMyPGIUr0z0EPrO65nNzJu

# **GANs for Abstractive Text Summarization**
## **NLP Group Project**
## **Statistical Natural Language Processing (COMP0087), University College London**

<hr>

**Project description**

A lot of endeavours have already been devoted to NLP text summarization techniques, and abstractive methods have proved to be more proficient in generating human-like sentences. At the same time, GANs has been enjoying considerable success in the area of real-valued data such as an image generation. Recently, researchers have begun to come up with ideas on how to overcome various obstacles during training GAN models for discrete data, though not a lot of work seemed to be directly dedicated to the text summarization itself. We, therefore, would like to pursue to tackle the issue of text summarization using the GAN techniques inspired by sources enlisted below.

<hr>

**Collaborators**

- Daniel Stancl (daniel.stancl.19@ucl.ac.uk)
- Dorota Jagnesakova (dorota.jagnesakova.19@ucl.ac.uk)
- Guolinag HE (guoliang.he.19@ucl.ac.uk)
- Zakhar Borok

# **1 Setup**

<hr>

- install and import libraries
- download stopwords
- remove and clone the most recent version of git repository
- run a script with a CONTRACTION_MAP
- run a script with a function for text preprocessing

### **GitHub stuff**

**Set GitHub credentials and username of repo owner**
"""

# credentials
user_email = 'dannyi@seznam.cz'
user = "gansforlife"
user_password = "dankodorkamichaelzak"

# username of repo owner
owner_username = 'stancld'
# reponame
reponame = 'NLP_Project'

# generate 
add_origin_link = (
    'https://{}:{}github@github.com/{}/{}.git'.format(
    user, user_password, owner_username, reponame)
)

print("Link used for git cooperation:\n{}".format(add_origin_link))

"""**Clone GitHub repo on the personal drive**"""

# Commented out IPython magic to ensure Python compatibility.
## Clone GitHub repo to the desired folder
# Mount drive
from google.colab import drive
drive.mount("/content/drive", force_remount = True)
# %cd "drive/My Drive/projects"

# Remove NLP_Project if presented and clone up-to-date repo
!rm -r NLP_Project
!git clone https://github.com/stancld/NLP_Project.git

# Go to the NLP_Project folder
# %cd NLP_Project

# Config global user and add origin enabling us to execute push commands
!git config --global user.email user_email
!git remote rm origin
!git remote add origin https://gansforlife:dankodorkamichaelzakgithub@github.com/stancld/NLP_Project.git

"""**Function push_to_repo**"""

def push_to_repo(file, file_path):
  """
  models_branch
  """
  np.savetxt(fname = 'Trained_models/{}.txt'.format(file_path), X = file)
  !git remote rm origin
  !git remote add origin https://gansforlife:dankodorkamichaelzak@github.com/stancld/NLP_Project.git
  !git checkout master
  !git pull origin master
  !git branch models_branch
  !git checkout models_branch
  !git add .
  !git commit -m "weight updates"
  !git checkout master
  !git merge models_branch
  !git push -u origin master

"""### **General stuff**

**Import essential libraries and load necessary conditionalities**
"""

import numpy as np
import pandas as pd
import re
import unicodedata

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

import nltk
from nltk.tokenize.toktok import ToktokTokenizer
from nltk.corpus import stopwords

from gensim.models import Word2Vec

nltk.download('stopwords')

"""**Set essential parameters**"""

# Set torch.device to use GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(torch.cuda.get_device_name())

"""**Run python files from with classes used throughtout the document**"""

run Code/contractions.py

run Code/text_preprocessing.py

"""### **Pretrained embeddings**

<hr>

**TODO:** *Put a comment which kind of embeddings we used. Add some references and so on*
"""

# Download and unzip GloVe embedding
!wget http://nlp.stanford.edu/data/glove.6B.zip
!unzip glove.6B.zip

# input your pre-train txt path and parse the data
path = 'glove.6B.100d.txt'
embed_dict = {}
with open(path,'r') as f:
  lines = f.readlines()
  for l in lines:
    w = l.split()[0]
    v = np.array(l.split()[1:]).astype('float')
    embed_dict[w] = v

embed_dict['@@_unknown_@@'] = np.zeros(100) # if we use 100 dimension embeddings

# remove all the unnecesary files
!rm -rf glove.6B.zip
!rm -rf glove.6B.50d.txt
!rm -rf glove.6B.100d.txt
!rm -rf glove.6B.200d.txt
!rm -rf glove.6B.300d.txt

# check the length of the dictionary
len(embed_dict.keys())

"""**Function for extracting relevant matrix of pretrained weights**"""

def extract_weight(text_dictionary):
  """
  :param text_dictionary:
  """
  pre_train_weight = []
  for word_index in text_dictionary.index2word.keys():
    if word_index != 0:
      word = text_dictionary.index2word[word_index]
      try:
        word_vector = embed_dict[word].reshape(1,-1)
      except:
        word_vector = embed_dict['@@_unknown_@@'].reshape(1,-1) # handle unknown word
      pre_train_weight = np.vstack([pre_train_weight,word_vector])
    else:
      word = text_dictionary.index2word[word_index]
      try:
        word_vector = embed_dict[word].reshape(1,-1)
      except:
        word_vector = embed_dict['@@_unknown_@@'].reshape(1,-1) # handle unknown word
      pre_train_weight = word_vector
  return pre_train_weight

"""# **2 Load and process the data**

<hr>

**Source of the data:** https://ucsb.app.box.com/s/7yq601ijl1lzvlfu4rjdbbxforzd2oag

##### *Download and open data*
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# !wget https://public.boxcloud.com/d/1/b1!IasNSF3ztSNJdWJRV3LSpFmVfMnM72cL7uMIWuqbuP8sLKlzFh-DWJtmsCo88GpAoGuaBEGmyyIi8XySuUR7002zIS4zLw1wUAay3fx9EEl2-LXGRY3tQm985E_7Kh__z2JpcSIfD2d1aqRybp4vRPafJ1Vd0F2r3HQ_rbqux9zwKvL1w3qAOLP9HIPjeAl_hht51e9GN66KVR8Gt3PkrMGUKwD5yoDzK-mjFt57a5150dKN8dqTQw_vvHPubi-KtZR6ABgcvwwIZFlGID6tz8hPq5_T95zDw5lvkueftrRqpQjJDCP0c1KL3aIiIaR4D6CWbvabwwl5y_h0G755RguIsj4GC9UJrJq6vb25A-qSzwocdi0b-ZOED4aUF6AKaBWCuBHcH2quJur-2hlpE78K1RwzC-SdBkt44Bw-lF0FqsZb9NEckw86qWE_06hZ761v8CS5QbTNwIUswaupK5ATTRT95Cghk9JoVMqtVTLNBApsf88o8C4DJvIeWsst3I1E8R62djoHr7KAandKNPDH627mYGKoYWahSvbm2WolnDEFJGR03QZ1YmbVG8n3ZCIOP99boNztepY3tsXfmE6QKBX186QegeRrcSA4kvxTMWPyR0klbAhw_iFlx1fKuaeON0sxPAzAUqKNKamaMHer1235wshTbSLtdM6TMRQqEd_DWnBJFEyOE2PFvBbQKYhV_1EJksixuXqROEGVHG7bP-S5YNhHL7pPDn6f1miAcSJvI-9APK6Lfsy0EHAZp1FQ7Adn0gzosJbVH7kx4CUKOM5XtGKkFmWgBYPRPgtlQhtgXyO2Y1OBMpCvaBkxeOt4I1zAgdYjekSaE6SqftVgIdOuM5oxFORpnPhbMsy65HLPynHVU4ijFUzUD6EaiGVuYSvlPuG2Yd7YiQNXlDUlwXM_lIMLWBt77IWB-68ENNJBYUciSqlnoeHj2n4UExLdam8Nf2RebqJumCyA_PfftOS8LizcnLnyo7Ek1wU01zkAZyKnXO-4S4ykVf_3Im8-kVCIsHJhLvDypnl4biJSL3iaIRfJGymyG67tx7NNOtAdCHZ4517J69efhA5z4HbQFMQSPvaQKNDWAJWOJ8IXxNED_EDSA2N7OQTkCDIGz9G6EYyw_1aGfgqqzI1Erofcc9q0Lwn39lWwrkappvykP6RJzWnHDnu6N_lTdCMgdGNrFQrMUtwWpZ0mCLsKToUGcSgfrvPxrMuz6lLZyySgXlrN6hWZIR23hElwCSKAZsJcXg../download
# 
# data = pd.read_csv('download',
#                    error_bad_lines = False).astype(str)
# print(data.shape)

"""##### *Clean flawed examples*

<hr>

- drop examples where text is shorter than headline
"""

text_len = [len(str(t)) for t in data.text]
head_len = [len(str(h)) for h in data.headline]

diff = [t - h for t, h in zip(text_len, head_len)]

problems = [problem for problem, d in enumerate(diff) if (d < 0) & (problem in data.index)]

data = data.drop(index = problems)

print(data.shape)

"""##### *Pre-process data*"""

for item in ['text', 'headline']:
  exec("""{}_data = text_preprocessing(data=data, item = '{}', contraction_map=CONTRACTION_MAP,
                                  drop_digits=False, remove_stopwords=False, stemming=False)""".format(item, item),
       locals(), globals()
  )

"""##### *Split data into train/val/test set*

<hr>

It's crucial to do this split in this step so that a dictionary that will be created for our model won't contain any words from validation/test set which are not presented in the training data.
"""

np.random.seed(222)

split = np.random.uniform(0, 1, size = data.shape[0])

# Train set
text_train, headline_train = text_data[split <= 0.8], headline_data[split <= 0.8]
# Validation set
text_val, headline_val = text_data[(split > 0.8) & (split <= 0.9)], headline_data[(split > 0.8) & (split <= 0.9)]
# Test set
text_test, headline_test = text_data[split > 0.9], headline_data[split > 0.9]

"""##### *Sort dataset from the longest sequence to the shortest one*"""

def sort_data(text, headline):
  """
  """
  headline = np.array(
      [y for x,y in sorted(zip(text, headline), key = lambda pair: len(pair[0]), reverse = True)]
  )
  text = list(text)
  text.sort(key = lambda x: len(x), reverse = True)
  text = np.array(text)

  return text, headline

# Train set
text_train, headline_train = sort_data(text_train, headline_train)
# Validation set
text_val, headline_val = sort_data(text_val, headline_val)
# Test set
text_test, headline_test = sort_data(text_test, headline_test)

"""### **Prepare dictionary and embeddings**

##### *Create a dictionary and prepare a digestible representation of the data*
"""

class LangDict:
  """
  Source: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html
  """
  def __init__(self):
    self.word2index = {}
    self.word2count = {}
    self.index2word = {0: "<SOS>", 1: "<EOS>"}
    self.n_words = 2

  def add_article(self, article):
    for word in article:
      self.add_word(word)

  def add_word(self, word):
    if word not in self.word2index:
      self.word2index[word] = self.n_words
      self.word2count[word] = 1
      self.index2word[self.n_words] = word
      self.n_words += 1
    else:
      self.word2count[word] += 1

# Create dictionary based on the training data
text_dictionary = LangDict()

for article in text_train:
  text_dictionary.add_article(article)

for article in headline_train:
  text_dictionary.add_article(article)

### ---- TO BE ADDED ---- ####

# Trim a dictionary to 50k & 150k most frequently used words
print(len(text_dictionary.word2index.keys()))

"""##### *Extract embedding vectors for words we need*"""



"""# **3 Training**

<hr>
 
-
"""