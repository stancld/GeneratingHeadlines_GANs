{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GANs for Abstractive Text Summarization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ult0blXNbHtv",
        "colab_type": "text"
      },
      "source": [
        "**Function keeping Colab running**\n",
        "\n",
        "<hr>\n",
        "\n",
        "1. Ctrl + Shift + I\n",
        "2. Put the code below into the console"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70kWNWiMbHdG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "49da3173-c218-4d04-cef9-56c8d2cd0004"
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "function ConnectButton(){\n",
        "    console.log(\"Connect pushed\"); \n",
        "    document.querySelector(\"#connect\").click() \n",
        "}\n",
        "setInterval(ConnectButton,60000)\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nfunction ConnectButton(){\\n    console.log(\"Connect pushed\"); \\n    document.querySelector(\"#connect\").click() \\n}\\nsetInterval(ConnectButton,60000)\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIyt-AKk1biq",
        "colab_type": "text"
      },
      "source": [
        "# **GANs for Abstractive Text Summarization**\n",
        "## **NLP Group Project**\n",
        "## **Statistical Natural Language Processing (COMP0087), University College London**\n",
        "\n",
        "<hr>\n",
        "\n",
        "**Project description**\n",
        "\n",
        "A lot of endeavours have already been devoted to NLP text summarization techniques, and abstractive methods have proved to be more proficient in generating human-like sentences. At the same time, GANs has been enjoying considerable success in the area of real-valued data such as an image generation. Recently, researchers have begun to come up with ideas on how to overcome various obstacles during training GAN models for discrete data, though not a lot of work seemed to be directly dedicated to the text summarization itself. We, therefore, would like to pursue to tackle the issue of text summarization using the GAN techniques inspired by sources enlisted below.\n",
        "\n",
        "<hr>\n",
        "\n",
        "**Collaborators**\n",
        "\n",
        "- Daniel Stancl (daniel.stancl.19@ucl.ac.uk)\n",
        "- Dorota Jagnesakova (dorota.jagnesakova.19@ucl.ac.uk)\n",
        "- Guolinag HE (guoliang.he.19@ucl.ac.uk)\n",
        "- Zakhar Borok"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzNR-0NJoCOV",
        "colab_type": "text"
      },
      "source": [
        "# **1 Setup**\n",
        "\n",
        "<hr>\n",
        "\n",
        "- install and import libraries\n",
        "- download stopwords\n",
        "- remove and clone the most recent version of git repository\n",
        "- run a script with a CONTRACTION_MAP\n",
        "- run a script with a function for text preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRRMxZxggWVl",
        "colab_type": "text"
      },
      "source": [
        "### **GitHub stuff**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkaEqWkNphsX",
        "colab_type": "text"
      },
      "source": [
        "**Set GitHub credentials and username of repo owner**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0r58gXnphct",
        "colab_type": "code",
        "outputId": "0bc9c612-193f-45ba-aa63-b932a6467d1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# credentials\n",
        "user_email = 'dannyi@seznam.cz'\n",
        "user = \"gansforlife\"\n",
        "user_password = \"dankodorkamichaelzak\"\n",
        "\n",
        "# username of repo owner\n",
        "owner_username = 'stancld'\n",
        "# reponame\n",
        "reponame = 'GeneratingHeadlines_GANs'\n",
        "\n",
        "# generate \n",
        "add_origin_link = (\n",
        "    'https://{}:{}github@github.com/{}/{}.git'.format(\n",
        "    user, user_password, owner_username, reponame)\n",
        ")\n",
        "\n",
        "print(\"Link used for git cooperation:\\n{}\".format(add_origin_link))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Link used for git cooperation:\n",
            "https://gansforlife:dankodorkamichaelzakgithub@github.com/stancld/GeneratingHeadlines_GANs.git\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Izq91t63kr2l",
        "colab_type": "text"
      },
      "source": [
        "**Clone GitHub repo on the personal drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "va3lSdh1ycMI",
        "colab_type": "code",
        "outputId": "1b6c6fec-53ce-4590-8821-1f104cb9e32d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "## Clone GitHub repo to the desired folder\n",
        "# Mount drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount = True)\n",
        "%cd \"drive/My Drive/projects\"\n",
        "\n",
        "# Remove NLP_Project if presented and clone up-to-date repo\n",
        "!rm -r GeneratingHeadlines_GANs\n",
        "!git clone https://github.com/stancld/GeneratingHeadlines_GANs.git\n",
        "\n",
        "# Go to the NLP_Project folder\n",
        "%cd GeneratingHeadlines_GANs\n",
        "\n",
        "# Config global user and add origin enabling us to execute push commands\n",
        "!git config --global user.email user_email\n",
        "!git remote rm origin\n",
        "!git remote add origin https://gansforlife:dankodorkamichaelzakgithub@github.com/stancld/GeneratingHeadlines_GANs.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/projects\n",
            "Cloning into 'GeneratingHeadlines_GANs'...\n",
            "remote: Enumerating objects: 63, done.\u001b[K\n",
            "remote: Counting objects: 100% (63/63), done.\u001b[K\n",
            "remote: Compressing objects: 100% (41/41), done.\u001b[K\n",
            "remote: Total 1172 (delta 45), reused 37 (delta 20), pack-reused 1109\u001b[K\n",
            "Receiving objects: 100% (1172/1172), 16.74 MiB | 18.70 MiB/s, done.\n",
            "Resolving deltas: 100% (807/807), done.\n",
            "/content/drive/My Drive/projects/GeneratingHeadlines_GANs\n",
            "CPU times: user 219 ms, sys: 83.9 ms, total: 303 ms\n",
            "Wall time: 11.1 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vtd82jejgjsJ",
        "colab_type": "text"
      },
      "source": [
        "**Function push_to_repo**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQKbDCp2gZs1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def push_to_repo():\n",
        "  \"\"\"\n",
        "  models_branch\n",
        "  \"\"\"\n",
        "  !git remote rm origin\n",
        "  !git remote add origin https://gansforlife:dankodorkamichaelzak@github.com/stancld/GeneratingHeadlines_GANs.git\n",
        "  !git checkout master\n",
        "  !git pull origin master\n",
        "  !git branch models_branch\n",
        "  !git checkout models_branch\n",
        "  !git add .\n",
        "  !git commit -m \"model state update\"\n",
        "  !git checkout master\n",
        "  !git merge models_branch\n",
        "  !git push -u origin master"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSITDO2N88Mu",
        "colab_type": "text"
      },
      "source": [
        "### **General stuff**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQViomvJmmxy",
        "colab_type": "text"
      },
      "source": [
        "**Import essential libraries and load necessary conditionalities**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfSL4JZGoXH_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "import gc\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import re\n",
        "import unicodedata\n",
        "import nltk\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9EI-8lbFAfk",
        "colab_type": "code",
        "outputId": "4bc19077-0b47-40e4-f890-3b3ad2b2d32b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9csn_FoXmqMR",
        "colab_type": "text"
      },
      "source": [
        "**Set essential parameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fpfj2zejmsaV",
        "colab_type": "code",
        "outputId": "e65c272a-65c8-451b-f78c-aea551b96549",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Set torch.device to use GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(torch.cuda.get_device_name())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJyjpXDMo-HN",
        "colab_type": "text"
      },
      "source": [
        "**Run python files from with classes used throughtout the document**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhIgA7osp7Hj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "run Code/contractions.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mb3DjRcT1gm-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# code for text_preprocessing()\n",
        "run Code/text_preprocessing.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZ0x45Yzqggm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# code for transforming data to padded array\n",
        "run Code/data2PaddedArray.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBm-0p9IS9yq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# code for the baseline model class _Seq2Seq()\n",
        "run Code/Models/Attention_seq2seq.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrL0zprqccwk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# code for the training class\n",
        "run Code/Models/generator_training_class.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZMHJLms80hC",
        "colab_type": "text"
      },
      "source": [
        "### **Pretrained embeddings**\n",
        "\n",
        "<hr>\n",
        "\n",
        "**TODO:** *Put a comment which kind of embeddings we used. Add some references and so on*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfPzKeHh80DB",
        "colab_type": "code",
        "outputId": "b1ccca2c-ef36-4853-fb75-4285cd5776a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Download and unzip GloVe embedding\n",
        "#!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "#!unzip glove.6B.zip\n",
        "\n",
        "\n",
        "# input your pre-train txt path and parse the data\n",
        "path = '../data/glove.6B.100d.txt'\n",
        "embed_dict = {}\n",
        "with open(path,'r') as f:\n",
        "  lines = f.readlines()\n",
        "  for l in lines:\n",
        "    w = l.split()[0]\n",
        "    v = np.array(l.split()[1:]).astype('float')\n",
        "    embed_dict[w] = v\n",
        "\n",
        "embed_dict['@@_unknown_@@'] = np.random.random(100) # if we use 100 dimension embeddings\n",
        "\n",
        "# remove all the unnecesary files\n",
        "#!rm -rf glove.6B.zip\n",
        "#!rm -rf glove.6B.50d.txt\n",
        "#!rm -rf glove.6B.100d.txt\n",
        "#!rm -rf glove.6B.200d.txt\n",
        "#!rm -rf glove.6B.300d.txt\n",
        "\n",
        "# check the length of the dictionary\n",
        "len(embed_dict.keys())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "400001"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_doqN6K-L_f",
        "colab_type": "text"
      },
      "source": [
        "**Function for extracting relevant matrix of pretrained weights** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwSHzoth-LmA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_weight(text_dictionary):\n",
        "  \"\"\"\n",
        "  :param text_dictionary:\n",
        "  \"\"\"\n",
        "  pre_train_weight = []\n",
        "  for word_index in text_dictionary.index2word.keys():\n",
        "    if word_index != 0:\n",
        "      word = text_dictionary.index2word[word_index]\n",
        "      try:\n",
        "        word_vector = embed_dict[word].reshape(1,-1)\n",
        "      except:\n",
        "        word_vector = embed_dict['@@_unknown_@@'].reshape(1,-1) # handle unknown word\n",
        "      pre_train_weight = np.vstack([pre_train_weight,word_vector])\n",
        "    \n",
        "    # add for padding\n",
        "    elif word_index == len(text_dictionary.index2word.keys()):  \n",
        "      pre_train_weight = np.r_[pre_train_weight, np.zeros((1, 100))]\n",
        "    \n",
        "    else:\n",
        "      word = text_dictionary.index2word[word_index]\n",
        "      try:\n",
        "        word_vector = embed_dict[word].reshape(1,-1)\n",
        "      except:\n",
        "        word_vector = embed_dict['@@_unknown_@@'].reshape(1,-1) # handle unknown word\n",
        "      pre_train_weight = word_vector\n",
        "  return pre_train_weight"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25PaDFvYp8VY",
        "colab_type": "text"
      },
      "source": [
        "# **2 Load and process the data**\n",
        "\n",
        "<hr>\n",
        "\n",
        "**Source of the data:** https://ucsb.app.box.com/s/7yq601ijl1lzvlfu4rjdbbxforzd2oag"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1AQ45P8lRzX",
        "colab_type": "text"
      },
      "source": [
        "##### *Download and open data*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRY-oCuJLC-u",
        "colab_type": "code",
        "outputId": "8ed764f6-90bd-4fbb-d7ca-2836abb37e75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "data = pd.read_csv('../data/wikihowSep.csv',\n",
        "                   error_bad_lines = False).astype(str)\n",
        "print(data.shape)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1585695, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tn-AsxLZVvOD",
        "colab_type": "text"
      },
      "source": [
        "##### *Clean flawed examples*\n",
        "\n",
        "<hr>\n",
        "\n",
        "- drop examples based on the threshold"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKyHNUTrWNvL",
        "colab_type": "code",
        "outputId": "c5cb3def-cc94-42d0-9498-77e8945247e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "max_examples = 50000\n",
        "max_threshold = 0.75\n",
        "min_threshold = 0.75\n",
        "\n",
        "# drop examples with an invalid ratio of length of text and headline\n",
        "text_len = [len(str(t)) for t in data.text]\n",
        "head_len = [len(str(h)) for h in data.headline]\n",
        "\n",
        "ratio = [h/t for t, h in zip(text_len, head_len)]\n",
        "\n",
        "problems1 = [problem for problem, r in enumerate(ratio) if (r > max_threshold) & (r < min_threshold) & (problem in data.index)]\n",
        "data = data.drop(index = problems1).reset_index().drop('index', axis = 1)\n",
        "\n",
        "# drop too short and long articles (to avoid struggles with CUDA memory)\n",
        "text_len = [len(str(t)) for t in data.text]\n",
        "\n",
        "problems2 = [problem for problem, text_len in enumerate(ratio) if (text_len > 600) & (text_len <= 100) & (problem in data.index)]\n",
        "data = data.drop(index = problems2)\n",
        "\n",
        "# some cleaning\n",
        "del text_len, head_len, ratio, problems1, problems2\n",
        "gc.collect()\n",
        "\n",
        "# trim the data to have only a subset of the data for our project\n",
        "try:\n",
        "  data = data[:max_examples]\n",
        "except:\n",
        "  pass\n",
        "\n",
        "print(data.shape)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(100000, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pK8sAfrOlXlG",
        "colab_type": "text"
      },
      "source": [
        "##### *Pre-process data*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_Mhr5-d1aqx",
        "colab_type": "code",
        "outputId": "fc6df603-1d15-4d8f-bf16-681e51daa639",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "%time\n",
        "\n",
        "for item in ['text', 'headline']:\n",
        "  exec(\"\"\"{}_data = text_preprocessing(data=data, item = '{}', contraction_map=CONTRACTION_MAP,\n",
        "                                  drop_digits=False, remove_stopwords=False, stemming=False)\"\"\".format(item, item),\n",
        "       locals(), globals()\n",
        "  )"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
            "Wall time: 7.87 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBkI74Qa9Y9v",
        "colab_type": "text"
      },
      "source": [
        "##### *Split data into train/val/test set*\n",
        "\n",
        "<hr>\n",
        "\n",
        "It's crucial to do this split in this step so that a dictionary that will be created for our model won't contain any words from validation/test set which are not presented in the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsXTsEEV8851",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(222)\n",
        "\n",
        "split = np.random.uniform(0, 1, size = data.shape[0])\n",
        "\n",
        "# Train set\n",
        "text_train, headline_train = text_data[split <= 0.9], headline_data[split <= 0.9]\n",
        "# Validation set\n",
        "text_val, headline_val = text_data[(split > 0.9) & (split <= 0.95)], headline_data[(split > 0.9) & (split <= 0.95)]\n",
        "# Test set\n",
        "text_test, headline_test = text_data[split > 0.95], headline_data[split > 0.95]\n",
        "\n",
        "del data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0nqBWooWsRv",
        "colab_type": "text"
      },
      "source": [
        "##### *Sort dataset from the longest sequence to the shortest one*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4tHv5huWss9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sort_data(text, headline):\n",
        "  \"\"\"\n",
        "  \"\"\"\n",
        "  headline = np.array(\n",
        "      [y for x,y in sorted(zip(text, headline), key = lambda pair: len(pair[0]), reverse = True)]\n",
        "  )\n",
        "  text = list(text)\n",
        "  text.sort(key = lambda x: len(x), reverse = True)\n",
        "  text = np.array(text)\n",
        "\n",
        "  return text, headline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olWbPN8eY8j9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train set\n",
        "text_train, headline_train = sort_data(text_train, headline_train)\n",
        "# Validation set\n",
        "text_val, headline_val = sort_data(text_val, headline_val)\n",
        "# Test set\n",
        "text_test, headline_test = sort_data(text_test, headline_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TLaKPwlFHdu",
        "colab_type": "text"
      },
      "source": [
        "### **Prepare dictionary and embeddings**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dfcRmfIN5Yc",
        "colab_type": "text"
      },
      "source": [
        "##### *Create a dictionary and prepare a digestible representation of the data*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbH1F24FOOhe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LangDict:\n",
        "  \"\"\"\n",
        "  Source: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    self.word2index = {}\n",
        "    self.word2count = {}\n",
        "    self.index2word = {0: \"sos\", 1: \"eos\"}\n",
        "    self.n_words = 2\n",
        "\n",
        "  def add_article(self, article):\n",
        "    for word in article:\n",
        "      self.add_word(word)\n",
        "\n",
        "  def add_word(self, word):\n",
        "    if word not in self.word2index:\n",
        "      self.word2index[word] = self.n_words\n",
        "      self.word2count[word] = 1\n",
        "      self.index2word[self.n_words] = word\n",
        "      self.n_words += 1\n",
        "    else:\n",
        "      self.word2count[word] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4p5hFIFLQz_G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create dictionary based on the training data\n",
        "text_dictionary = LangDict()\n",
        "\n",
        "for article in text_train:\n",
        "  text_dictionary.add_article(article)\n",
        "\n",
        "for article in headline_train:\n",
        "  text_dictionary.add_article(article)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IULBuowRDHM",
        "colab_type": "code",
        "outputId": "0f8280c5-4764-40ed-d7db-6da682dc8c00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print(\"There are {:.0f} distinct words in the untrimmed dictionary\".format(len(text_dictionary.word2index.keys())))\n",
        "\n",
        "# Trim a dictionary to the words with at least 10 occurences within the text\n",
        "min_count = 3\n",
        "subset_words = [word for (word, count) in text_dictionary.word2count.items() if count >= min_count]\n",
        "text_dictionary.word2index = {word: i for (word, i) in zip(subset_words, range(len(subset_words)))}\n",
        "text_dictionary.index2word = {i: word for (word, i) in zip(subset_words, range(len(subset_words)))}\n",
        "text_dictionary.word2count = {word: count for (word, count) in text_dictionary.word2count.items() if count >= min_count}\n",
        "\n",
        "print(\"There are {:.0f} distinct words in the trimmed dictionary, where only word with at least {:.0f} occurences are retained\".format(len(text_dictionary.word2index.keys()), min_count))\n",
        "del min_count, subset_words"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 54708 distinct words in the untrimmed dictionary\n",
            "There are 29794 distinct words in the trimmed dictionary, where only word with at least 3 occurences are retained\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "992ZzrRcoK7d",
        "colab_type": "text"
      },
      "source": [
        "*Add pad token*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_TTnlQ5oKl2",
        "colab_type": "code",
        "outputId": "4c1033de-3ebd-4139-8a1f-886b2a892c0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "pad_idx = max(list(text_dictionary.index2word.keys())) + 1\n",
        "\n",
        "text_dictionary.word2index['<pad>'] = pad_idx\n",
        "text_dictionary.index2word[pad_idx] = '<pad>'\n",
        "\n",
        "print(len(text_dictionary.index2word.keys()))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "29795\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGQcSyqKHF5l",
        "colab_type": "text"
      },
      "source": [
        "##### *Extract embedding vectors for words we need*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iTKyHy0lQvw",
        "colab_type": "code",
        "outputId": "8fbec4ab-4460-497a-f4fb-f277aa723016",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "%%time\n",
        "pre_train_weight = extract_weight(text_dictionary)\n",
        "pre_train_weight = np.array(pre_train_weight, dtype = np.float32)\n",
        "\n",
        "del embed_dict\n",
        "gc.collect()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1min 1s, sys: 3.02 s, total: 1min 4s\n",
            "Wall time: 1min 4s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHL6SrbtjSZX",
        "colab_type": "text"
      },
      "source": [
        "### **Transform the data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONMMcJytjS47",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train set\n",
        "text_train, text_lengths_train, headline_train, headline_lengths_train = data2PaddedArray(text_train, headline_train, text_dictionary, pre_train_weight)\n",
        "# Validation set\n",
        "text_val, text_lengths_val, headline_val, headline_lengths_val = data2PaddedArray(text_val, headline_val, text_dictionary, pre_train_weight)\n",
        "# Test set\n",
        "text_test, text_lengths_test, headline_test, headline_lengths_test = data2PaddedArray(text_test, headline_test, text_dictionary, pre_train_weight)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3N4c4riOR_Ck",
        "colab_type": "text"
      },
      "source": [
        "# **3 Training**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZF6q63I5XpZN",
        "colab_type": "text"
      },
      "source": [
        "## **3.1 Generator - Pretraining**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kGCefrdXpMM",
        "colab_type": "text"
      },
      "source": [
        "### **3.1.1 Version - 1**\n",
        "\n",
        "<hr>\n",
        "\n",
        "**Description**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUkoLDKlmDkL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grid = {'max_epochs': 100,\n",
        "        'batch_size': 64,\n",
        "        'learning_rate': 5e-4,\n",
        "        'clip': 10,\n",
        "        'l2_reg': 1e-4,\n",
        "        'model_name': \"generator01\"\n",
        "      }\n",
        "\n",
        "##### model ######\n",
        "OUTPUT_DIM = len(text_dictionary.index2word.keys())\n",
        "ENC_EMB_DIM = 100\n",
        "ENC_HID_DIM = 128\n",
        "DEC_HID_DIM = 128\n",
        "\n",
        "ENC_DROPOUT = 0\n",
        "DEC_DROPOUT = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93vbQiZ7lABE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Generator = generator(model = _Seq2Seq, loss_function = nn.CrossEntropyLoss, optimiser = optim.Adam, l2_reg = grid['l2_reg'], batch_size = grid['batch_size'],\n",
        "                      text_dictionary = text_dictionary, embeddings = pre_train_weight, max_epochs = grid['max_epochs'], learning_rate = grid['learning_rate'],\n",
        "                      clip = grid['clip'], teacher_forcing_ratio = 0.5, OUTPUT_DIM = OUTPUT_DIM, ENC_HID_DIM = ENC_HID_DIM, ENC_EMB_DIM = ENC_EMB_DIM,\n",
        "                      DEC_HID_DIM = DEC_HID_DIM, ENC_DROPOUT = ENC_DROPOUT, DEC_DROPOUT = DEC_DROPOUT, device = device, model_name = grid['model_name'],\n",
        "                      push_to_repo = push_to_repo)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zuZnRR4a9iK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Generator.load()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTosh2_RmxiA",
        "colab_type": "code",
        "outputId": "c7a93efc-6c78-45d7-c994-33898f7bf882",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        }
      },
      "source": [
        "Generator.train(X_train = text_train,\n",
        "                y_train = headline_train,\n",
        "                X_val = text_val,\n",
        "                y_val = headline_val,\n",
        "                X_train_lengths = text_lengths_train,\n",
        "                y_train_lengths = headline_lengths_train,\n",
        "                X_val_lengths = text_lengths_val,\n",
        "                y_val_lengths = headline_lengths_val)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 - Intermediate loss 5.352 after 0.19 % of training examples.\n",
            "Epoch 1 - Intermediate loss 5.592 after 0.38 % of training examples.\n",
            "Epoch 1 - Intermediate loss 5.574 after 0.57 % of training examples.\n",
            "Epoch 1 - Intermediate loss 5.536 after 0.75 % of training examples.\n",
            "Epoch 1 - Intermediate loss 5.506 after 0.89 % of training examples.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-72c8c5c005bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                 \u001b[0my_train_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mheadline_lengths_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                 \u001b[0mX_val_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_lengths_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                 y_val_lengths = headline_lengths_val)\n\u001b[0m",
            "\u001b[0;32m/content/drive/My Drive/projects/GeneratingHeadlines_GANs/Code/Models/generator_training_class.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X_train, y_train, X_val, y_val, X_train_lengths, y_train_lengths, X_val_lengths, y_val_lengths)\u001b[0m\n\u001b[1;32m    205\u001b[0m                 \u001b[0;31m### BACKWARD PASS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;31m# Make update step w.r.t. clipping gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clip'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimiser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzalxXCmpK10",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YD34v6StqA-T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ly69sV-t15NQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2HWGXtwayhe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhcXSU8-wCjH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nU_PgAn2wazh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}