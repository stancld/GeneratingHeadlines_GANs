{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "c39KzsJgQmw_",
    "outputId": "2a502478-533c-428b-d633-b250b67c2457"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'temp'...\n",
      "remote: Enumerating objects: 7, done.\u001b[K\n",
      "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
      "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
      "remote: Total 7 (delta 1), reused 0 (delta 0), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (7/7), done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 1,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "import random\n",
    "#increase field limit to read embedding\n",
    "import sys\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "!git clone https://github.com/guol1nag/temp.git\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "0PuxpDIIRYAi",
    "outputId": "61cdba1b-2df0-42d8-b0d1-48b060edb3c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sentence: 100\n",
      "number of tokens e.g.: 13309\n"
     ]
    }
   ],
   "source": [
    "path = r'./temp/toydata.pickle'\n",
    "\n",
    "with open(path, 'rb') as f:\n",
    "  data = pickle.load(f)\n",
    "train = data[0]\n",
    "test = data[1]\n",
    "X_train =[]\n",
    "X_test =[]\n",
    "y_train =[]\n",
    "y_test =[]\n",
    "allsentences = []\n",
    "for i in range(len(train)):\n",
    "  X_train.append(train[i][0])\n",
    "  y_train.append(train[i][1])\n",
    "  X_test.append(test[i][0])\n",
    "  y_test.append(test[i][1])\n",
    "\n",
    "print('number of sentence:',len(X_train))\n",
    "print('number of tokens e.g.:',len(X_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hy8eHaSAj3vD"
   },
   "outputs": [],
   "source": [
    "class LangDict:\n",
    "  def __init__(self):\n",
    "    self.word2index = {}\n",
    "    self.word2count = {}\n",
    "    self.index2word = {0: '<sos>', 1: '<eos>'}\n",
    "    self.n_words = 2\n",
    "\n",
    "  def add_article(self, article):\n",
    "    for word in article:\n",
    "      self.add_word(word)\n",
    "\n",
    "  def add_word(self, word):\n",
    "    if word not in self.word2index:\n",
    "      self.word2index[word] = self.n_words\n",
    "      self.word2count[word] = 1\n",
    "      self.index2word[self.n_words] = word\n",
    "      self.n_words += 1\n",
    "    else:\n",
    "      self.word2count[word] += 1\n",
    "\n",
    "text_dictionary = LangDict()\n",
    "\n",
    "for article in X_train:\n",
    "  text_dictionary.add_article(article)\n",
    "\n",
    "for article in y_train:\n",
    "  text_dictionary.add_article(article)\n",
    "\n",
    "for article in X_test:\n",
    "  text_dictionary.add_article(article)\n",
    "\n",
    "for article in y_test:\n",
    "  text_dictionary.add_article(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Tp5IwwhRQYy"
   },
   "source": [
    "# pre-train embedding\n",
    "\n",
    "pretrained_weight is a numpy matrix of shape (num_embeddings, embedding_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "colab_type": "code",
    "id": "5ewkZnSMcm5c",
    "outputId": "eed883b9-7ae1-4423-845a-d72a34e76f9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-02-25 11:44:16--  http://nlp.stanford.edu/data/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
      "--2020-02-25 11:44:16--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
      "--2020-02-25 11:44:16--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: ‘glove.6B.zip’\n",
      "\n",
      "glove.6B.zip        100%[===================>] 822.24M  2.03MB/s    in 6m 28s  \n",
      "\n",
      "2020-02-25 11:50:45 (2.12 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
      "\n",
      "Archive:  glove.6B.zip\n",
      "  inflating: glove.6B.50d.txt        \n",
      "  inflating: glove.6B.100d.txt       \n",
      "  inflating: glove.6B.200d.txt       \n",
      "  inflating: glove.6B.300d.txt       \n",
      "CPU times: user 30.4 s, sys: 1.89 s, total: 32.3 s\n",
      "Wall time: 7min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip glove.6B.zip\n",
    "\n",
    "# input your pre-train txt path\n",
    "path = '/content/glove.6B.100d.txt'\n",
    "embed_dict = {}\n",
    "with open(path,'r') as f:\n",
    "  lines = f.readlines()\n",
    "  for l in lines:\n",
    "    w = l.split()[0]\n",
    "    v = np.array(l.split()[1:]).astype('float')\n",
    "    embed_dict[w] = v\n",
    "\n",
    "embed_dict['@@_unknown_@@'] = np.zeros(100) # if we use 100 dimension embeddings\n",
    "\n",
    "!rm -rf glove.6B.zip\n",
    "!rm -rf glove.6B.50d.txt\n",
    "!rm -rf glove.6B.100d.txt\n",
    "!rm -rf glove.6B.200d.txt\n",
    "!rm -rf glove.6B.300d.txt\n",
    "\n",
    "len(embed_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "Yp8jBbRVl7Do",
    "outputId": "ad36b3f4-e1d9-4adb-e68e-eabcc347014f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26008, 100)\n",
      "CPU times: user 1min 42s, sys: 4.41 s, total: 1min 47s\n",
      "Wall time: 1min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pre_train_weight = []\n",
    "for word_index in text_dictionary.index2word.keys():\n",
    "  if word_index != 0:\n",
    "    word = text_dictionary.index2word[word_index]\n",
    "    try:\n",
    "      word_vector = embed_dict[word].reshape(1,-1)\n",
    "    except:\n",
    "      word_vector = embed_dict['@@_unknown_@@'].reshape(1,-1) # handle unknown word\n",
    "    pre_train_weight = np.vstack([pre_train_weight,word_vector])\n",
    "  else:\n",
    "    word = text_dictionary.index2word[word_index]\n",
    "    try:\n",
    "      word_vector = embed_dict[word].reshape(1,-1)\n",
    "    except:\n",
    "      word_vector = embed_dict['@@_unknown_@@'].reshape(1,-1) # handle unknown word\n",
    "    pre_train_weight = word_vector\n",
    "\n",
    "#embed = nn.Embedding.from_pretrained(torch.from_numpy(pre_train_weight),freeze=True)\n",
    "print(pre_train_weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w-JLG0wUtyxR"
   },
   "source": [
    "# pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zrffsk02feOA"
   },
   "outputs": [],
   "source": [
    "class helper(): #helper functions\n",
    "  @staticmethod\n",
    "  def article_list_2_index_tensor(article_list: list) -> torch.Tensor:\n",
    "    article_list = sorted(article_list,key=lambda x:len(x),reverse=True)\n",
    "    longest = len(article_list[0])\n",
    "    for i,article in enumerate(article_list):\n",
    "      index = torch.Tensor([text_dictionary.word2index[token] for token in article])\n",
    "      padding = longest - len(index)\n",
    "      index = torch.cat([index,torch.zeros(padding)]).long().view(1,-1)   ########## need to change <sos> padding to <eos>\n",
    "      index_list = (index if i == 0 else torch.cat((index_list,index),dim=0))\n",
    "    return index_list\n",
    "\n",
    "  @staticmethod\n",
    "  def batcher(iterable: torch.Tensor,\\\n",
    "              label: torch.Tensor, \\\n",
    "              batch_size: int = 1) -> torch.Tensor:\n",
    "    l = len(iterable)\n",
    "    for batch in range(0, l, batch_size):\n",
    "        yield (iterable[batch:min(batch + batch_size, l)],label[batch:min(batch + batch_size, l)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ej0MfN6KtZ2z",
    "outputId": "5a8ef567-c8d2-4346-db6b-6f5446477fc7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 13309])"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = helper.article_list_2_index_tensor(X_train)\n",
    "y_train = helper.article_list_2_index_tensor(y_train)\n",
    "X_test = helper.article_list_2_index_tensor(X_test)\n",
    "y_test = helper.article_list_2_index_tensor(y_test)\n",
    "X_train.size() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N5QeMH8ORamz"
   },
   "source": [
    "# seq2seq\n",
    "\n",
    "see: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html#training-the-model\n",
    "\n",
    "RNNs are inherently sequential. They are auto-regressive, meaning the input for timestep t contains the output for timestep (t-1), meaning you have to first calculae the output for timestap (t-1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JrjxccHVuraD"
   },
   "source": [
    "##### encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_eM-2p2nRcyE"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  def __init__(self,pre_train_weight,batch_size):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.hidden_state = pre_train_weight.shape[1] # dimension of word vectors\n",
    "    self.input_size = pre_train_weight.shape[0]   # number of all words\n",
    "    self.batch_size = batch_size\n",
    "\n",
    "    self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(pre_train_weight),freeze=True)\n",
    "    \n",
    "    self.gru = nn.GRU(input_size=self.hidden_state,hidden_size=self.hidden_state,\\\n",
    "                                  num_layers=1)\n",
    "    \n",
    "    self.ini_hidden = torch.zeros(1, self.batch_size, self.hidden_state)\n",
    "\n",
    "  def forward(self, input):\n",
    "    embedded = self.embedding(input).float()\n",
    "    input_to_rnn = embedded.transpose(0,1)\n",
    "    (output,hidden) = self.gru(input_to_rnn,self.ini_hidden)\n",
    "    return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ONT1QGgPdaaS",
    "outputId": "4f2bc3db-c3ac-4310-b4b3-27d2550f4963"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 13309, 100])"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_batch = torch.cat((X_train[0].unsqueeze(0),X_train[1].unsqueeze(0)),dim=0)\n",
    "\n",
    "embedding = nn.Embedding.from_pretrained(torch.from_numpy(pre_train_weight),freeze=True)\n",
    "embedding(mini_batch).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "JQqTEVF-3dSx",
    "outputId": "b4528996-b555-4eba-fb21-12e0e17d94d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 13309])\n",
      "encoder out torch.Size([13309, 2, 100]) torch.Size([1, 2, 100])\n"
     ]
    }
   ],
   "source": [
    "#test batching 2 \n",
    "mini_batch = torch.cat((X_train[0].unsqueeze(0),X_train[1].unsqueeze(0)),dim=0)\n",
    "print(mini_batch.size())\n",
    "\n",
    "encoder = Encoder(pre_train_weight,batch_size=2).to(device)\n",
    "encoder_output,encode_out_hidden = encoder(mini_batch)\n",
    "print('encoder out',encoder_output.size(),encode_out_hidden.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7wxQwCxTutcM"
   },
   "source": [
    "##### decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zU3KP2rS_hVA"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "  def __init__(self,hidden_state: 'hidden state in decoder RNN'):\n",
    "    super(Attention, self).__init__()\n",
    "    self.attn = nn.Linear(hidden_state * 2, hidden_state) # here assumre hidden = input size\n",
    "    self.attn_weight_coe = nn.Parameter(torch.rand(hidden_state))\n",
    "    \n",
    "  def forward(self,hidden,encoder_outputs):\n",
    "    seq_len = encoder_outputs.size(0)\n",
    "    atten_hidden = hidden.unsqueeze(0).\\\n",
    "                      repeat(seq_len, 1, 1).transpose(0, 1) # (batch_size,seq_len,hidden)\n",
    "    encoder_outputs = encoder_outputs.transpose(0, 1)  # (batch_size,seq_len,hidden)\n",
    "\n",
    "    attn = F.relu(self.attn(torch.cat([atten_hidden, encoder_outputs],\\\n",
    "                          dim=2))).transpose(1, 2)  #[batch_size,hidden,seq_len]\n",
    "    attn_weight = self.attn_weight_coe.repeat(encoder_outputs.size(0),\\\n",
    "                                1).unsqueeze(1) # [batch_size,1,hidden]\n",
    "    attention = torch.bmm(attn_weight,attn).squeeze(1) # [batch_size,seq_len]\n",
    "    scored_attn = F.softmax(attention, dim=1).unsqueeze(1)\n",
    "    return scored_attn # **[batch_size,1,seq_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uf0USW5vuqtw"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "  def __init__(self,pre_train_weight: np.array,batch_size,dropout_p=0.1):\n",
    "    super(Decoder, self).__init__()\n",
    "\n",
    "    self.hidden_state = pre_train_weight.shape[1]\n",
    "    self.output_size = pre_train_weight.shape[1]\n",
    "    self.dropout_p = dropout_p\n",
    "    self.batch_size = batch_size\n",
    "\n",
    "    self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(pre_train_weight),freeze=True)\n",
    "\n",
    "    self.attention = Attention(self.hidden_state)\n",
    "\n",
    "    self.dropout = nn.Dropout(self.dropout_p)\n",
    "    self.gru = nn.GRU(input_size = self.hidden_state * 2 , hidden_size = self.hidden_state)\n",
    "    self.out = nn.Linear(self.hidden_state*2, self.output_size)\n",
    "\n",
    "  def forward(self, input: 'index of a single word, i.e. len is 1',\\\n",
    "                           prev_hidden, encoder_outputs): # N = sequence length for output! hidden = feature size\n",
    "    # Get the embedding of the current input word (last output word)\n",
    "    embedded = self.embedding(input).float()  # (batch_size,hidden)\n",
    "    embedded = self.dropout(embedded).view(1,self.batch_size,-1) # (1,batch_size,hidden)\n",
    "\n",
    "    scored_attn = self.attention(prev_hidden[-1],encoder_outputs) # **[batch_size,1,seq_len]\n",
    "    encoder_outputs = encoder_outputs.transpose(0, 1)   # [batch_size,seq_len,hidden]\n",
    "    context = torch.bmm(scored_attn,encoder_outputs)    # [batch_size,1,hidden]\n",
    "    context = context.transpose(0, 1)  # [1,batch_size,hidden]\n",
    "\n",
    "    rnn_input = torch.cat([embedded, context],dim=2) # [1,batch_size,hidden*2] \n",
    "\n",
    "    rnn_output, hidden = self.gru(rnn_input, prev_hidden) \n",
    "\n",
    "    # format rnn_output\n",
    "    rnn_output = rnn_output.squeeze(0)  \n",
    "    context = context.squeeze(0)\n",
    "    output = self.out(torch.cat([rnn_output, context], dim = 1))\n",
    "    output = F.log_softmax(output, dim=1)    # [batch_size,output_size]\n",
    "    return (output, hidden, scored_attn)\n",
    "\n",
    "  def get_decoder_hidden(self,encoder_hidden): # from encode ouput hidden to decoder initial hidden\n",
    "    return(encoder_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "colab_type": "code",
    "id": "i7pds5ep6Ax9",
    "outputId": "73b57060-440b-48bb-ba17-098be0e37b55"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-bffd19dee201>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdecoder_in_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_decoder_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencode_out_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdec_out\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdec_hidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdec_attn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_in_hidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoder_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdec_hidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdec_attn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dec_input' is not defined"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(pre_train_weight,batch_size=2,dropout_p=0.1)\n",
    "encoder_outputs = encoder_output\n",
    "decoder_in_hidden = decoder.get_decoder_hidden(encode_out_hidden)\n",
    "\n",
    "dec_out,dec_hidden,dec_attn = decoder(dec_input, decoder_in_hidden,encoder_outputs)\n",
    "print(dec_out.size(),dec_hidden.size(),dec_attn.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2kvtu9BlfH56"
   },
   "source": [
    "seq 2 seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M9vrx4C7asDn"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "  def __init__(self, encoder, decoder):\n",
    "    super(Seq2Seq, self).__init__()\n",
    "    self.encoder = encoder\n",
    "    self.decoder = decoder\n",
    "  \n",
    "  def forward(self, train_input: '[seq_len,batch_size,input_size]',\\\n",
    "              target: '[seq_len,batch_size,input_size]',\\\n",
    "              teacher_forcing_ratio=0.5):\n",
    "    batch_size = train_input.size(1)\n",
    "    max_len = target.size(0)      # may be slightly longer?\n",
    "    vocab_size = self.decoder.output_size # = word embedding space? \n",
    "\n",
    "    # encoder \n",
    "    # encoder_output.size[seq_len,batch_size,input_size]\n",
    "    # enc_out_hidden.size[1,batch_size,input_size]\n",
    "    encoder_output, enc_out_hidden = self.encoder(train_input) # enc_ini_hidden assume to be 0\n",
    "\n",
    "    dec_hidden = self.decoder.get_decoder_hidden(enc_out_hidden[:1]) # hidden[:1].size(batch_size,hidden)\n",
    "\n",
    "    dec_output = torch.zeros(max_len, batch_size, vocab_size).to(device)\n",
    "    temp_output = target.data[0, :]  # TODO must the first ouput = <sos> index, size(batch_size,hidden)\n",
    "\n",
    "    #decoder   haven't checked yet\n",
    "    for t in range(1, max_len):\n",
    "      temp_output, dec_hidden, attn_weights = self.decoder(\n",
    "              temp_output, dec_hidden, encoder_output)\n",
    "      dec_output[t] = temp_output\n",
    "      is_teacher = random.random() < teacher_forcing_ratio\n",
    "      top1 = temp_output.data.max(1)[1]\n",
    "      temp_output = (target.data[t] if is_teacher else top1).to(device)\n",
    "    return dec_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HT5JI0jOtKCG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2PvxAw6GsLXC",
    "outputId": "2b8ddbaf-a59f-4c94-cb11-85527a3c77e1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 168,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = (200 if 10 > 1 else 100)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mfiNUNsKi5Oi",
    "outputId": "0976ec59-3f1e-4678-c857-1c4b875e0e77"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29170238144321825"
      ]
     },
     "execution_count": 174,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "uCQZO4vVmXtA",
    "outputId": "5b85f523-7c43-4efb-ae24-5489695eb8f9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<sos>'"
      ]
     },
     "execution_count": 155,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_dictionary.index2word[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "BdwDHexBo8Qw",
    "outputId": "82dedff0-f657-4be2-9a5b-5d2270118cea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<sos>'"
      ]
     },
     "execution_count": 156,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_dictionary.index2word[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "hczN_4G6p02s",
    "outputId": "94beaaa4-1b2b-4ce0-a1fc-ced1913459e1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 157,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_dictionary.word2index['<sos>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vj-qEyWRqRoG",
    "outputId": "34cca055-e44d-4d87-b189-0eabef60ce02"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<eos>'"
      ]
     },
     "execution_count": 158,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_dictionary.index2word[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fsjZW7m-qXE8",
    "outputId": "5f4df47f-eaed-49c1-9856-e06e812f0621"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<eos>'"
      ]
     },
     "execution_count": 159,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_dictionary.index2word[25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "lOGAcQJrqYUO",
    "outputId": "c6c170e9-f561-483d-deb8-c2a88d33e2c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 160,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_dictionary.word2index['<eos>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Loxa9-UaqbXe"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "9Tp5IwwhRQYy"
   ],
   "name": "generator.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
