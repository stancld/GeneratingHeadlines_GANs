{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"New_attn_seq2seq.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMF2HQ6GNot82p1wmG/eEQf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"c39KzsJgQmw_","colab_type":"code","outputId":"68ba4e24-3a79-436f-fee0-a5a6b4762f1b","executionInfo":{"status":"ok","timestamp":1583004065327,"user_tz":0,"elapsed":12385,"user":{"displayName":"Michael River","photoUrl":"","userId":"13207336642017563139"}},"colab":{"base_uri":"https://localhost:8080/","height":136}},"source":["import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import pickle\n","import random\n","import re\n","import sklearn.model_selection as sk_ModelSelection\n","#increase field limit to read embedding\n","import sys\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","!git clone https://github.com/guol1nag/temp.git\n","torch.cuda.get_device_name()"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Cloning into 'temp'...\n","remote: Enumerating objects: 13, done.\u001b[K\n","remote: Counting objects: 100% (13/13), done.\u001b[K\n","remote: Compressing objects: 100% (13/13), done.\u001b[K\n","remote: Total 13 (delta 3), reused 0 (delta 0), pack-reused 0\u001b[K\n","Unpacking objects: 100% (13/13), done.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["'Tesla T4'"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"code","metadata":{"id":"SOfnGMEyaqbF","colab_type":"code","outputId":"bbb2b550-80d2-424b-d9d6-4f6cd412d5f8","executionInfo":{"status":"ok","timestamp":1583004065875,"user_tz":0,"elapsed":12924,"user":{"displayName":"Michael River","photoUrl":"","userId":"13207336642017563139"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["train_path = r'./temp/toy_X.csv'\n","test_path = r'./temp/toy_Y.csv'\n","\n","X = [re.sub(\"'\", \"\", x.strip('[').strip('[')).split(', ') for x in pd.read_csv(train_path, squeeze = True).tolist()]\n","y = [re.sub(\"'\", \"\", x.strip('[').strip('[')).split(', ') for x in pd.read_csv(test_path, squeeze = True).tolist()]\n","\n","X_train,X_test,y_train,y_test = sk_ModelSelection.train_test_split(X,y,test_size=0.2)\n","\n","print('number of sentence:',len(X_train))\n","print('number of tokens e.g.:',len(X_train[0]))\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["number of sentence: 80\n","number of tokens e.g.: 988\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Hy8eHaSAj3vD","colab_type":"code","colab":{}},"source":["class LangDict:\n","  def __init__(self):\n","    self.word2index = {}\n","    self.word2count = {}\n","    self.index2word = {0: '<sos>', 1: '<eos>'}\n","    self.n_words = 2\n","\n","  def add_article(self, article):\n","    for word in article:\n","      self.add_word(word)\n","\n","  def add_word(self, word):\n","    if word not in self.word2index:\n","      self.word2index[word] = self.n_words\n","      self.word2count[word] = 1\n","      self.index2word[self.n_words] = word\n","      self.n_words += 1\n","    else:\n","      self.word2count[word] += 1\n","\n","text_dictionary = LangDict()\n","\n","for article in X_train:\n","  text_dictionary.add_article(article)\n","\n","for article in y_train:\n","  text_dictionary.add_article(article)\n","\n","for article in X_test:\n","  text_dictionary.add_article(article)\n","\n","for article in y_test:\n","  text_dictionary.add_article(article)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9Tp5IwwhRQYy","colab_type":"text"},"source":["# pre-train embedding & pre-processing\n","\n","pretrained_weight is a numpy matrix of shape (num_embeddings, embedding_dim)\n"]},{"cell_type":"code","metadata":{"id":"5ewkZnSMcm5c","colab_type":"code","outputId":"122c59f4-9fc1-4bf8-c337-5fb20a857438","executionInfo":{"status":"ok","timestamp":1583004511747,"user_tz":0,"elapsed":458786,"user":{"displayName":"Michael River","photoUrl":"","userId":"13207336642017563139"}},"colab":{"base_uri":"https://localhost:8080/","height":476}},"source":["%%time\n","\n","!wget http://nlp.stanford.edu/data/glove.6B.zip\n","!unzip glove.6B.zip\n","\n","# input your pre-train txt path\n","path = '/content/glove.6B.100d.txt'\n","embed_dict = {}\n","with open(path,'r') as f:\n","  lines = f.readlines()\n","  for l in lines:\n","    w = l.split()[0]\n","    v = np.array(l.split()[1:]).astype('float')\n","    embed_dict[w] = v\n","\n","embed_dict['@@_unknown_@@'] = np.zeros(100) # if we use 100 dimension embeddings\n","\n","!rm -rf glove.6B.zip\n","!rm -rf glove.6B.50d.txt\n","!rm -rf glove.6B.100d.txt\n","!rm -rf glove.6B.200d.txt\n","!rm -rf glove.6B.300d.txt\n","\n","len(embed_dict.keys())"],"execution_count":4,"outputs":[{"output_type":"stream","text":["--2020-02-29 19:21:06--  http://nlp.stanford.edu/data/glove.6B.zip\n","Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n","Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n","--2020-02-29 19:21:06--  https://nlp.stanford.edu/data/glove.6B.zip\n","Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n","--2020-02-29 19:21:06--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n","Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n","Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 862182613 (822M) [application/zip]\n","Saving to: ‘glove.6B.zip’\n","\n","glove.6B.zip        100%[===================>] 822.24M  2.17MB/s    in 6m 26s  \n","\n","2020-02-29 19:27:32 (2.13 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n","\n","Archive:  glove.6B.zip\n","  inflating: glove.6B.50d.txt        \n","  inflating: glove.6B.100d.txt       \n","  inflating: glove.6B.200d.txt       \n","  inflating: glove.6B.300d.txt       \n","CPU times: user 26 s, sys: 1.2 s, total: 27.3 s\n","Wall time: 7min 26s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Yp8jBbRVl7Do","colab_type":"code","outputId":"b994f4fe-6496-4dcd-f779-ffa89b8aef0a","executionInfo":{"status":"ok","timestamp":1583004522208,"user_tz":0,"elapsed":467442,"user":{"displayName":"Michael River","photoUrl":"","userId":"13207336642017563139"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["%%time\n","pre_train_weight = []\n","for word_index in text_dictionary.index2word.keys():\n","  if word_index != 0:\n","    word = text_dictionary.index2word[word_index]\n","    try:\n","      word_vector = embed_dict[word].reshape(1,-1)\n","    except:\n","      word_vector = embed_dict['@@_unknown_@@'].reshape(1,-1) # handle unknown word\n","    pre_train_weight = np.vstack([pre_train_weight,word_vector])\n","  else:\n","    word = text_dictionary.index2word[word_index]\n","    try:\n","      word_vector = embed_dict[word].reshape(1,-1)\n","    except:\n","      word_vector = embed_dict['@@_unknown_@@'].reshape(1,-1) # handle unknown word\n","    pre_train_weight = word_vector\n","\n","#embed = nn.Embedding.from_pretrained(torch.from_numpy(pre_train_weight),freeze=True)\n","print(pre_train_weight.shape)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["(12914, 100)\n","CPU times: user 9.58 s, sys: 608 ms, total: 10.2 s\n","Wall time: 10.2 s\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"N5QeMH8ORamz","colab_type":"text"},"source":["\n","# seq2seq\n","\n","see: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html#training-the-model\n","\n","RNNs are inherently sequential. They are auto-regressive, meaning the input for timestep t contains the output for timestep (t-1), meaning you have to first calculae the output for timestap (t-1)."]},{"cell_type":"markdown","metadata":{"id":"JrjxccHVuraD","colab_type":"text"},"source":["##### encoder"]},{"cell_type":"code","metadata":{"id":"_eM-2p2nRcyE","colab_type":"code","colab":{}},"source":["class Encoder(nn.Module):\n","  def __init__(self, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n","    super().__init__()\n","    self.rnn = nn.GRU(input_size = emb_dim, hidden_size = enc_hid_dim, bidirectional = True)\n","    self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n","    self.dropout = nn.Dropout(dropout)\n","    \n","  def forward(self, enc_input):\n","    \n","    #enc_input = [enc_input_len, batch size,emb_dim]\n","    \n","    embedded = self.dropout(enc_input)\n","    \n","    #embedded = [enc_input_len, batch size, emb_dim]\n","    \n","    outputs, hidden = self.rnn(embedded)\n","            \n","    #outputs = [enc_input len, batch size, hid dim * num directions]\n","    #hidden = [n layers * num directions, batch size, hid dim]\n","    \n","    #hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n","    #outputs are always from the last layer\n","    \n","    #hidden [-2, :, : ] is the last of the forwards RNN \n","    #hidden [-1, :, : ] is the last of the backwards RNN\n","    \n","    #initial decoder hidden is final hidden state of the forwards and backwards \n","    #  encoder RNNs fed through a linear layer\n","    hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n","    \n","    #outputs = [src len, batch size, enc hid dim * 2]\n","    #hidden = [batch size, dec hid dim]\n","    return outputs, hidden"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7wxQwCxTutcM","colab_type":"text"},"source":["##### decoder"]},{"cell_type":"code","metadata":{"id":"zU3KP2rS_hVA","colab_type":"code","colab":{}},"source":["class Attention(nn.Module):\n","  def __init__(self, enc_hid_dim, dec_hid_dim):\n","    super().__init__()\n","    \n","    self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n","    self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n","      \n","  def forward(self, hidden, encoder_outputs):\n","    \n","    #hidden = [batch size, dec hid dim]\n","    #encoder_outputs = [src len, batch size, enc hid dim * 2]\n","    \n","    batch_size = encoder_outputs.shape[1]\n","    src_len = encoder_outputs.shape[0]\n","    \n","    #repeat decoder hidden state src_len times\n","    hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n","    \n","    encoder_outputs = encoder_outputs.permute(1, 0, 2)\n","    \n","    #hidden = [batch size, src len, dec hid dim]\n","    #encoder_outputs = [batch size, src len, enc hid dim * 2]\n","    \n","    energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) \n","    \n","    #energy = [batch size, src len, dec hid dim]\n","\n","    attention = self.v(energy).squeeze(2)\n","    \n","    #attention= [batch size, src len]\n","  \n","    return F.softmax(attention, dim=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Uf0USW5vuqtw","colab_type":"code","colab":{}},"source":["class Decoder(nn.Module):\n","  def __init__(self, output_dim,enc_hid_dim,  dec_hid_dim, dropout, attention):\n","    super().__init__()\n","\n","    self.output_dim = output_dim\n","    self.attention = attention\n","    \n","    #self.embedding = nn.Embedding(output_dim, output_dim)\n","    \n","    self.rnn = nn.GRU((enc_hid_dim * 2) + output_dim, dec_hid_dim)\n","    \n","    self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + output_dim, output_dim)\n","    \n","    self.dropout = nn.Dropout(dropout)\n","      \n","  def forward(self, dec_input, hidden, encoder_outputs):\n","          \n","    #dec_input = [1,batch size,dec_emb dim]\n","    #hidden = [batch size, dec hid dim]\n","    #encoder_outputs = [src len, batch size, enc hid dim * 2]\n","            \n","    embedded = self.dropout(dec_input)\n","    \n","    #embedded = [1, batch size, dec_emb dim]\n","    \n","    a = self.attention(hidden, encoder_outputs)\n","\n","    #a = [batch size, src len]\n","    \n","    a = a.unsqueeze(1)\n","    \n","    #a = [batch size, 1, src len]\n","    \n","    encoder_outputs = encoder_outputs.permute(1, 0, 2)\n","    \n","    #encoder_outputs = [batch size, src len, enc hid dim * 2]\n","    \n","    weighted = torch.bmm(a, encoder_outputs)\n","    \n","    #weighted = [batch size, 1, enc hid dim * 2]\n","    \n","    weighted = weighted.permute(1, 0, 2)\n","    #weighted = [1, batch size, enc hid dim * 2]\n","\n","    # print('embedded',embedded.size())\n","    rnn_input = torch.cat((embedded, weighted), dim = 2)\n","    \n","    #rnn_input = [1, batch size, (enc hid dim * 2) + dec_emb dim]\n","        \n","    output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n","    \n","    #output = [seq len, batch size, dec hid dim * n directions]\n","    #hidden = [n layers * n directions, batch size, dec hid dim]\n","    \n","    #seq len, n layers and n directions will always be 1 in this decoder, therefore:\n","    #output = [1, batch size, dec hid dim]\n","    #hidden = [1, batch size, dec hid dim]\n","    #this also means that output == hidden\n","    assert (output == hidden).all()\n","    \n","    embedded = embedded.squeeze(0)\n","    output = output.squeeze(0)\n","    weighted = weighted.squeeze(0)\n","    \n","    prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))\n","    \n","    #prediction = [batch size, output dim]\n","    \n","    return prediction, hidden.squeeze(0)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2kvtu9BlfH56","colab_type":"text"},"source":["seq 2 seq"]},{"cell_type":"code","metadata":{"id":"M9vrx4C7asDn","colab_type":"code","colab":{}},"source":["class Seq2Seq(nn.Module):\n","  def __init__(self, encoder, decoder, device):\n","      super().__init__()\n","      \n","      self.encoder = encoder\n","      self.decoder = decoder\n","      self.device = device\n","      \n","  def forward(self, seq2seq_input: '[seq_len, batch size,Enc_emb_dim]',\\\n","               target: '[trg_len, batch size,output_dim]',\\\n","               teacher_forcing_ratio = 0.5):\n","      \n","      #seq2seq_input = [seq_len, batch size,Enc_emb_dim]\n","      #target = [trg_len, batch size,output_dim]\n","      #teacher_forcing_ratio is probability to use teacher forcing\n","      #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n","\n","      batch_size = seq2seq_input.shape[1]\n","      trg_len = target.shape[0]\n","      trg_vocab_size = self.decoder.output_dim\n","      \n","      #tensor to store decoder dec_output\n","      dec_output = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n","      \n","      #encoder_outputs is all hidden states of the input sequence, back and forwards\n","      #hidden is the final forward and backward hidden states, passed through a linear layer\n","      encoder_outputs, hidden = self.encoder(seq2seq_input)\n","              \n","      # check: make dimension consistent \n","      dec_input = target[0]\n","      dec_input = dec_input.unsqueeze(0)\n","      \n","      # print('dec_input dim:',dec_input.size())\n","      \n","      for t in range(1, trg_len):\n","        #insert dec_input token embedding, previous hidden state and all encoder hidden states\n","        #receive output tensor (predictions) and new hidden state\n","        output, hidden = self.decoder(dec_input, hidden, encoder_outputs)\n","        \n","        #place predictions in a tensor holding predictions for each token\n","        dec_output[t] = output\n","        \n","        #decide if we are going to use teacher forcing or not\n","        teacher_force = random.random() < teacher_forcing_ratio\n","        \n","        #get the highest predicted token from our predictions\n","        top1 = output\n","\n","        #if teacher forcing, use actual next token as next input\n","        #if not, use predicted token\n","        dec_input = target[t] if teacher_force else top1\n","        dec_input = dec_input.unsqueeze(1).float()\n","\n","      return dec_output"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tCnvL2bPc4df","colab_type":"code","colab":{}},"source":["class utility(): #utility functions\n","  @staticmethod\n","  def article_list_2_index_tensor(article_list: list) -> torch.Tensor:\n","    article_list = sorted(article_list,key=lambda x:len(x),reverse=True)\n","    longest = len(article_list[0])\n","    for i,article in enumerate(article_list):\n","      index = torch.Tensor([text_dictionary.word2index[token] for token in article if token is str])\n","      padding = longest - len(index)\n","      index = torch.cat([index,torch.ones(padding)]).long().view(1,-1)   ########## TODO: need to change padding moode\n","      index_list = (index if i == 0 else torch.cat((index_list,index),dim=0))\n","    return index_list\n","    \n","  @staticmethod\n","  def batcher(iterable: torch.Tensor,\\\n","              label: torch.Tensor, \\\n","              batch_size: int = 1) -> torch.Tensor:\n","    l = len(iterable)\n","    for batch in range(0, l, batch_size):\n","        yield (iterable[batch:min(batch + batch_size, l)],label[batch:min(batch + batch_size, l)])\n","\n","  @staticmethod\n","  def Loss_opt(model, learning_rate):\n","    Lossfunc = torch.nn.CosineEmbeddingLoss().to(device)\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","    return(Lossfunc, optimizer)\n","\n","  @staticmethod\n","  def count_parameters(model):\n","      return sum(p.numel() for p in model.parameters() if p.requires_grad)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QkoCDOAibkNj","colab_type":"code","outputId":"9bb8c9f6-41ff-4125-ffb8-808867e6e4c9","executionInfo":{"status":"ok","timestamp":1583005828772,"user_tz":0,"elapsed":463,"user":{"displayName":"Michael River","photoUrl":"","userId":"13207336642017563139"}},"colab":{"base_uri":"https://localhost:8080/","height":323}},"source":["grid = {'max_epochs':3,\n","        'batch_size':1,\n","        'leanring_rate':1e-3,\n","        'clip':10\n","      }\n","\n","##### model ######\n","OUTPUT_DIM = 100\n","ENC_EMB_DIM = 100\n","#DEC_EMB_DIM = 1\n","ENC_HID_DIM = 128\n","DEC_HID_DIM = 128\n","ENC_DROPOUT = 0\n","DEC_DROPOUT = 0\n","\n","attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n","enc = Encoder(ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n","dec = Decoder(output_dim = OUTPUT_DIM,  enc_hid_dim = ENC_HID_DIM, dec_hid_dim = DEC_HID_DIM, dropout = DEC_DROPOUT,attention= attn)\n","model = Seq2Seq(enc, dec, device).to(device)\n","\n","###### embedding layer \n","embedding_layer = nn.Embedding.from_pretrained(torch.from_numpy(pre_train_weight),freeze=True).to(device)\n","\n","###### optimizer, lossfunction \n","lossfunction,optimiser = utility.Loss_opt(model,grid['leanring_rate'])\n","\n","\n","print(f'The model has {utility.count_parameters(model):,} trainable parameters')\n","print('')\n","model"],"execution_count":49,"outputs":[{"output_type":"stream","text":["The model has 494,068 trainable parameters\n","\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["Seq2Seq(\n","  (encoder): Encoder(\n","    (rnn): GRU(100, 128, bidirectional=True)\n","    (fc): Linear(in_features=256, out_features=128, bias=True)\n","    (dropout): Dropout(p=0, inplace=False)\n","  )\n","  (decoder): Decoder(\n","    (attention): Attention(\n","      (attn): Linear(in_features=384, out_features=128, bias=True)\n","      (v): Linear(in_features=128, out_features=1, bias=False)\n","    )\n","    (rnn): GRU(356, 128)\n","    (fc_out): Linear(in_features=484, out_features=100, bias=True)\n","    (dropout): Dropout(p=0, inplace=False)\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":49}]},{"cell_type":"code","metadata":{"id":"ct1KfNfKBqi2","colab_type":"code","colab":{}},"source":["def training(grid,X_train,y_train,model,lossfunction,optimiser):\n","\n","  #collect statistics\n","  train_loss =[]\n","  val_loss =[]\n","  best_loss = float('inf')\n","\n","  for epoch in range(grid['max_epochs']):\n","    trainloss = 0\n","\n","    for local_batch, local_labels in utility.batcher(X_train,y_train,grid['batch_size']):\n","      local_batch = utility.article_list_2_index_tensor(local_batch).to(device)\n","      local_labels = utility.article_list_2_index_tensor(local_labels).to(device)\n","\n","      # pass through embedding layer to become vectors\n","      local_batch = embedding_layer(local_batch).float().transpose(0,1)\n","      local_labels = embedding_layer(local_labels).float().transpose(0,1)\n","\n","      print('input tensor are:')\n","      print(local_batch.size())\n","      print(local_labels.size())\n","      \n","      optimiser.zero_grad()\n","      local_output = model(seq2seq_input = local_batch,target = local_labels,\\\n","                teacher_forcing_ratio=1)\n","      \n","      local_output = local_output.squeeze(1)\n","      local_labels = local_labels.squeeze(1)\n","\n","      print('output are:')\n","      print(local_output.size())\n","      print(local_labels.size())\n","\n","      loss = lossfunction(local_output,local_labels,torch.Tensor([1]).to(device)) # TODO figure out how to use this in matrix\n","\n","      print('loss:')\n","      print(loss.item())\n","      print('')\n","      \n","      trainloss += loss.item()\n","\n","      loss.backward()\n","      optimiser.step()\n","\n","      for i,param in enumerate(model.parameters()):\n","        if i == 12:\n","          print('')\n","          print('accu grad')\n","          print('')\n","          print(param.grad)\n","          print('')\n","          print('weight of a layer')\n","          print('')\n","          print(model.decoder.fc_out.weight)\n","          break\n","\n","    print(trainloss)\n","    train_loss.append(trainloss)\n","  return(train_loss)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JoQVxtMzZz6v","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"734ae57e-6341-43b9-b351-e5ecb51ef80a","executionInfo":{"status":"error","timestamp":1583005844947,"user_tz":0,"elapsed":6394,"user":{"displayName":"Michael River","photoUrl":"","userId":"13207336642017563139"}}},"source":["training(grid,X_train,y_train,model,lossfunction,optimiser)"],"execution_count":51,"outputs":[{"output_type":"stream","text":["input tensor are:\n","torch.Size([988, 1, 100])\n","torch.Size([7, 1, 100])\n","output are:\n","torch.Size([700])\n","torch.Size([700])\n","loss:\n","0.0015522119356319308\n","\n","\n","accu grad\n","\n","tensor([[-3.5280e-10, -7.3237e-09,  6.0441e-10,  3.2695e-09, -1.1592e-08,\n","          1.1562e-08,  2.5941e-09,  1.1581e-08, -2.4290e-09,  1.1274e-08,\n","          6.7235e-09,  4.5446e-09, -9.9300e-09, -4.8586e-10,  1.5775e-09,\n","         -7.3519e-09,  4.6500e-09,  1.9762e-08, -5.2684e-09,  2.5188e-09,\n","          1.5241e-09,  1.2968e-08,  5.4733e-09,  8.5555e-09,  2.6883e-09,\n","          2.1295e-09,  5.9762e-09, -4.6316e-10, -5.5479e-09, -3.4194e-10,\n","         -8.6947e-09, -6.0616e-09, -4.5247e-09, -2.9444e-09, -3.9826e-09,\n","         -1.9204e-09, -2.9745e-10, -5.7588e-09, -2.3542e-09, -1.4005e-09,\n","          9.2954e-10, -9.6939e-09, -4.4679e-09,  1.4347e-08,  2.6195e-09,\n","         -1.3070e-09, -1.6132e-10,  9.5288e-09,  2.1003e-09,  6.9331e-09,\n","          1.3579e-08, -5.3485e-09,  1.1747e-08, -2.9821e-09,  2.9438e-09,\n","         -1.1250e-08, -1.1611e-08,  1.2246e-08,  2.4122e-09,  8.5478e-09,\n","         -2.5070e-09, -9.2052e-09, -1.1996e-08, -2.2674e-09,  1.4098e-09,\n","          1.6044e-09,  4.3202e-09, -1.0111e-09,  1.8472e-10, -2.0170e-09,\n","          1.0946e-08,  5.0305e-10, -4.6505e-09, -3.7612e-09, -2.0366e-09,\n","          1.2478e-09,  9.8865e-09, -5.8637e-09,  5.4575e-09, -7.1214e-09,\n","          3.6902e-09, -3.5403e-09, -7.2819e-09, -2.1774e-11,  2.6299e-09,\n","         -6.6355e-09, -1.2612e-09,  5.2602e-09,  2.4908e-08,  1.3539e-09,\n","          4.0865e-09, -5.8480e-09,  6.2027e-09, -1.0334e-08,  9.1486e-09,\n","         -6.7206e-09, -7.8804e-09, -1.0002e-08, -1.2777e-08, -7.2832e-10,\n","         -1.1078e-08, -1.8930e-09, -2.3660e-09, -4.2409e-10,  3.5002e-09,\n","         -1.2774e-08, -1.0337e-09,  3.3050e-09, -9.7582e-09,  2.6856e-09,\n","         -1.6509e-11, -4.1509e-09, -1.0006e-08,  1.2853e-08,  3.0690e-09,\n","         -3.3060e-09, -4.3327e-10, -3.7458e-09,  6.4789e-09,  2.9562e-09,\n","          9.7841e-09,  6.2329e-09,  1.3779e-08, -1.3451e-09,  6.4291e-09,\n","         -2.3451e-09, -8.4550e-09, -2.7721e-09]], device='cuda:0')\n","\n","weight of a layer\n","\n","Parameter containing:\n","tensor([[ 0.0238, -0.0224,  0.0149,  ...,  0.0333,  0.0341,  0.0047],\n","        [-0.0427,  0.0315,  0.0153,  ...,  0.0038, -0.0283, -0.0025],\n","        [-0.0048,  0.0441, -0.0362,  ...,  0.0036,  0.0037, -0.0424],\n","        ...,\n","        [-0.0441, -0.0260,  0.0221,  ...,  0.0292,  0.0384,  0.0118],\n","        [-0.0273, -0.0086, -0.0135,  ...,  0.0164, -0.0310,  0.0148],\n","        [-0.0053, -0.0236, -0.0337,  ..., -0.0443,  0.0297, -0.0244]],\n","       device='cuda:0', requires_grad=True)\n","input tensor are:\n","torch.Size([2110, 1, 100])\n","torch.Size([9, 1, 100])\n","output are:\n","torch.Size([900])\n","torch.Size([900])\n","loss:\n","0.0004340146842878312\n","\n","\n","accu grad\n","\n","tensor([[-9.0829e-11, -2.7326e-10,  1.0369e-10,  2.1458e-10, -5.5386e-10,\n","          8.3622e-10,  1.7153e-10,  5.9150e-10, -2.8076e-11,  6.0799e-10,\n","          2.6743e-10,  9.9399e-11, -3.9202e-10, -2.2594e-11,  1.7222e-10,\n","         -5.0814e-10,  2.1675e-10,  1.0739e-09, -1.7068e-10,  1.4621e-10,\n","         -2.2665e-12,  4.3248e-10,  5.2417e-10,  7.5951e-10,  2.8904e-10,\n","          1.6701e-10,  4.3201e-10,  4.1029e-11, -6.8854e-10,  2.0829e-10,\n","         -5.0135e-10, -3.1986e-10, -7.9895e-11,  8.4755e-12, -1.3644e-10,\n","         -3.5694e-11,  4.6028e-11, -1.8523e-10, -1.4678e-10, -1.0570e-10,\n","         -4.7325e-11, -5.8082e-10, -1.3152e-10,  8.1262e-10,  2.3276e-10,\n","          7.6288e-12,  1.2981e-10,  3.5267e-10,  1.6516e-10,  2.7364e-10,\n","          6.6807e-10, -1.8996e-10,  6.8201e-10, -1.3128e-10, -5.8584e-12,\n","         -5.7549e-10, -6.0689e-10,  4.6371e-10, -3.2040e-11,  3.5724e-10,\n","         -2.8812e-10, -5.9088e-10, -4.0212e-10, -1.4508e-10,  9.8672e-12,\n","          1.8183e-10,  2.1875e-10, -4.0881e-11,  5.5691e-11, -8.8341e-11,\n","          6.0741e-10,  1.0545e-10, -1.0083e-10,  5.3573e-11, -5.1230e-11,\n","          1.6175e-10,  4.7382e-10, -3.8597e-10,  4.6766e-10, -3.8828e-10,\n","          7.7769e-11, -1.9717e-10, -5.2570e-10,  1.2415e-11,  3.0110e-10,\n","         -3.8710e-10,  5.3660e-11,  3.2259e-10,  1.2613e-09,  1.2654e-11,\n","          1.4857e-10, -2.2380e-10,  2.7645e-10, -4.5151e-10,  4.2827e-10,\n","         -3.4963e-10, -4.2529e-10, -6.8007e-10, -7.9075e-10,  4.0172e-11,\n","         -5.0308e-10, -4.5285e-11, -2.4748e-11, -2.8685e-11,  1.8238e-10,\n","         -7.1038e-10, -1.8973e-10,  7.7065e-11, -4.4215e-10,  3.5252e-10,\n","          1.3553e-10,  3.3338e-11, -5.1284e-10,  6.1439e-10, -8.1025e-11,\n","         -2.3249e-10, -1.4645e-10, -1.3785e-10,  3.5202e-10,  1.9079e-10,\n","          4.2821e-10,  2.2347e-10,  4.3241e-10, -4.8866e-11,  3.5267e-10,\n","         -1.0868e-10, -3.4784e-10, -2.2619e-10]], device='cuda:0')\n","\n","weight of a layer\n","\n","Parameter containing:\n","tensor([[ 0.0230, -0.0216,  0.0151,  ...,  0.0333,  0.0341,  0.0047],\n","        [-0.0419,  0.0306,  0.0150,  ...,  0.0038, -0.0283, -0.0025],\n","        [-0.0040,  0.0433, -0.0360,  ...,  0.0036,  0.0037, -0.0424],\n","        ...,\n","        [-0.0450, -0.0251,  0.0215,  ...,  0.0292,  0.0384,  0.0118],\n","        [-0.0282, -0.0078, -0.0139,  ...,  0.0164, -0.0310,  0.0148],\n","        [-0.0044, -0.0245, -0.0338,  ..., -0.0443,  0.0297, -0.0244]],\n","       device='cuda:0', requires_grad=True)\n","input tensor are:\n","torch.Size([1264, 1, 100])\n","torch.Size([12, 1, 100])\n","output are:\n","torch.Size([1200])\n","torch.Size([1200])\n","loss:\n","0.0001900488859973848\n","\n","\n","accu grad\n","\n","tensor([[-1.7855e-09, -1.6648e-09,  1.3119e-09,  2.4172e-09, -3.1304e-09,\n","          3.3203e-09,  1.2903e-09,  3.4220e-09,  1.2470e-09,  3.2538e-09,\n","          1.8692e-09,  2.4291e-10, -6.3726e-10,  1.7438e-11, -7.4872e-10,\n","         -3.2063e-09,  2.0717e-09,  5.3445e-09, -3.0099e-10,  4.9409e-10,\n","         -1.6086e-09,  1.5748e-09,  3.9326e-09,  5.0408e-09,  1.8249e-09,\n","         -8.4806e-11,  3.5280e-09,  8.9183e-10, -4.2639e-09,  6.4088e-10,\n","         -2.6601e-09, -6.2550e-10, -3.3228e-10,  1.6568e-10, -3.1978e-09,\n","         -3.3138e-10, -3.7145e-10,  1.0060e-10, -6.6937e-10, -6.0992e-10,\n","          1.0153e-09, -2.8665e-09, -1.8592e-09,  5.6180e-09, -2.1946e-10,\n","         -3.8434e-10,  2.2109e-09,  1.7337e-09,  1.3515e-09,  4.5121e-10,\n","          3.9207e-09,  7.6337e-11,  4.3664e-09, -1.2523e-09,  5.8312e-10,\n","         -3.5230e-09, -3.7684e-09,  2.3850e-09,  1.0077e-09,  1.5153e-09,\n","         -1.8377e-09, -3.6806e-09, -3.0278e-09, -2.3672e-09, -7.6846e-10,\n","          3.5273e-11,  7.6561e-10,  4.9605e-10,  1.4341e-09, -1.3205e-09,\n","          3.8887e-09,  1.2457e-09, -1.5981e-09,  2.0950e-09, -1.1188e-09,\n","          1.8769e-09,  2.3319e-09, -2.0738e-09,  2.8301e-09, -2.5851e-09,\n","         -1.8665e-10, -7.7207e-10, -3.2429e-09, -1.0207e-09,  2.2513e-09,\n","         -2.9203e-09,  1.4041e-09,  2.0657e-09,  7.7323e-09, -9.6003e-11,\n","         -3.1364e-11, -5.1401e-10,  1.3339e-09, -2.4605e-09,  2.7856e-09,\n","         -2.2381e-09, -1.3648e-09, -4.4082e-09, -5.6467e-09, -1.6349e-09,\n","         -3.7824e-09,  5.7681e-10,  1.7186e-10, -7.0305e-10,  1.2311e-09,\n","         -4.6259e-09, -1.0820e-09,  1.0063e-09, -1.9818e-09,  1.7845e-09,\n","          1.2983e-10,  6.9884e-10, -3.5075e-09,  4.4265e-09, -6.2399e-10,\n","         -2.3819e-09,  2.2574e-10, -1.5211e-10,  3.9528e-09,  3.7377e-10,\n","          3.1135e-09,  9.0601e-10,  2.5029e-09, -2.2013e-09,  2.4863e-09,\n","         -5.7736e-10, -1.5381e-09, -1.0897e-09]], device='cuda:0')\n","\n","weight of a layer\n","\n","Parameter containing:\n","tensor([[ 0.0224, -0.0210,  0.0151,  ...,  0.0333,  0.0341,  0.0047],\n","        [-0.0413,  0.0299,  0.0146,  ...,  0.0038, -0.0283, -0.0025],\n","        [-0.0034,  0.0427, -0.0358,  ...,  0.0036,  0.0037, -0.0424],\n","        ...,\n","        [-0.0457, -0.0245,  0.0209,  ...,  0.0292,  0.0384,  0.0118],\n","        [-0.0289, -0.0070, -0.0143,  ...,  0.0164, -0.0310,  0.0148],\n","        [-0.0038, -0.0252, -0.0341,  ..., -0.0443,  0.0297, -0.0244]],\n","       device='cuda:0', requires_grad=True)\n","input tensor are:\n","torch.Size([1339, 1, 100])\n","torch.Size([33, 1, 100])\n","output are:\n","torch.Size([3300])\n","torch.Size([3300])\n","loss:\n","0.00025428016670048237\n","\n","\n","accu grad\n","\n","tensor([[-3.6811e-09, -3.2285e-09,  2.8931e-09,  5.1559e-09, -5.6173e-09,\n","          4.9995e-09,  2.3735e-09,  6.0591e-09,  2.1462e-09,  6.0083e-09,\n","          3.2397e-09,  5.5659e-10, -1.0708e-09, -9.1691e-11, -2.1895e-09,\n","         -6.1690e-09,  4.0989e-09,  9.3608e-09,  1.5784e-11,  2.1689e-10,\n","         -2.8292e-09,  3.1789e-09,  7.6249e-09,  9.3366e-09,  3.3648e-09,\n","         -3.5157e-11,  7.0707e-09,  1.2112e-09, -7.1928e-09, -1.4756e-11,\n","         -4.4549e-09, -1.7261e-09, -1.4034e-09,  3.6495e-10, -6.7056e-09,\n","         -8.4878e-10, -1.2331e-09,  9.6645e-10, -5.9760e-10, -3.2325e-10,\n","          1.8006e-09, -5.3314e-09, -4.0311e-09,  1.0670e-08, -9.6107e-10,\n","         -1.4575e-09,  4.4728e-09,  4.0200e-09,  2.0590e-09,  9.7326e-10,\n","          7.2897e-09,  1.0957e-11,  7.9035e-09, -2.3457e-09,  1.7732e-09,\n","         -5.9998e-09, -6.3998e-09,  4.8791e-09,  2.6440e-09,  2.7177e-09,\n","         -4.0914e-09, -6.5466e-09, -6.8682e-09, -5.5432e-09, -1.0701e-09,\n","         -4.5685e-10,  2.3932e-09,  8.5318e-10,  2.6150e-09, -2.3611e-09,\n","          6.5280e-09,  2.6207e-09, -3.3153e-09,  4.1006e-09, -2.4413e-09,\n","          3.4526e-09,  4.0256e-09, -3.7065e-09,  4.0827e-09, -4.8464e-09,\n","         -3.9113e-10, -1.4938e-09, -5.4627e-09, -2.5961e-09,  4.6458e-09,\n","         -5.8004e-09,  2.9824e-09,  3.3781e-09,  1.5348e-08,  4.0356e-10,\n","          3.7707e-10, -7.6688e-10,  2.3169e-09, -4.0980e-09,  4.7810e-09,\n","         -3.7471e-09, -1.5957e-09, -8.2080e-09, -1.0871e-08, -3.9191e-09,\n","         -7.5152e-09,  1.5294e-09, -5.0905e-11, -1.3466e-09,  2.4041e-09,\n","         -9.4176e-09, -2.8821e-09,  1.9266e-09, -2.7611e-09,  2.8499e-09,\n","          3.9150e-11,  1.2760e-09, -6.6250e-09,  8.3153e-09, -8.8300e-10,\n","         -4.6843e-09,  1.0138e-09, -1.1003e-09,  7.6315e-09, -1.9281e-10,\n","          6.2044e-09,  1.6649e-09,  6.8396e-09, -5.6264e-09,  4.7457e-09,\n","         -2.1634e-09, -2.0811e-09, -3.6463e-10]], device='cuda:0')\n","\n","weight of a layer\n","\n","Parameter containing:\n","tensor([[ 0.0218, -0.0204,  0.0156,  ...,  0.0333,  0.0341,  0.0047],\n","        [-0.0407,  0.0293,  0.0140,  ...,  0.0038, -0.0283, -0.0025],\n","        [-0.0028,  0.0421, -0.0362,  ...,  0.0036,  0.0037, -0.0424],\n","        ...,\n","        [-0.0463, -0.0239,  0.0209,  ...,  0.0292,  0.0384,  0.0118],\n","        [-0.0295, -0.0065, -0.0144,  ...,  0.0164, -0.0310,  0.0148],\n","        [-0.0032, -0.0259, -0.0345,  ..., -0.0443,  0.0297, -0.0244]],\n","       device='cuda:0', requires_grad=True)\n","input tensor are:\n","torch.Size([1194, 1, 100])\n","torch.Size([5, 1, 100])\n","output are:\n","torch.Size([500])\n","torch.Size([500])\n","loss:\n","0.00018261866352986544\n","\n","\n","accu grad\n","\n","tensor([[-2.5817e-09, -2.4733e-09,  2.1117e-09,  3.8651e-09, -3.9616e-09,\n","          3.0137e-09,  1.6164e-09,  4.2184e-09,  1.0968e-09,  4.1596e-09,\n","          2.0390e-09,  5.1053e-10, -8.5779e-10, -3.2270e-10, -1.8439e-09,\n","         -4.2777e-09,  2.7393e-09,  6.4326e-09,  1.3636e-10, -2.2216e-10,\n","         -1.5121e-09,  2.6581e-09,  5.4092e-09,  6.3430e-09,  2.5944e-09,\n","          2.1211e-10,  5.2502e-09,  4.7012e-10, -4.4867e-09, -7.1928e-10,\n","         -2.8887e-09, -1.7154e-09, -1.5113e-09,  1.2726e-10, -4.6879e-09,\n","         -7.8328e-10, -1.1580e-09,  1.0696e-09, -7.4950e-11,  2.5881e-10,\n","          8.9685e-10, -4.0254e-09, -3.1072e-09,  7.5042e-09, -9.0262e-10,\n","         -1.4437e-09,  3.0179e-09,  3.3768e-09,  1.0567e-09,  1.0704e-09,\n","          4.9796e-09, -3.7031e-10,  5.3029e-09, -1.3908e-09,  1.6495e-09,\n","         -3.6308e-09, -4.0347e-09,  3.7332e-09,  2.2559e-09,  1.9636e-09,\n","         -3.2489e-09, -4.4414e-09, -5.4376e-09, -4.2877e-09, -4.1579e-10,\n","         -4.8811e-10,  2.2963e-09,  5.5413e-10,  1.5246e-09, -1.4549e-09,\n","          4.0198e-09,  1.9443e-09, -2.3594e-09,  2.6895e-09, -1.8187e-09,\n","          2.0710e-09,  2.7862e-09, -2.2481e-09,  2.1858e-09, -3.2995e-09,\n","         -2.3840e-10, -1.1759e-09, -3.3037e-09, -2.0458e-09,  3.2379e-09,\n","         -4.1155e-09,  1.9793e-09,  2.0008e-09,  1.1161e-08,  7.9146e-10,\n","          7.5680e-10, -5.7896e-10,  1.5795e-09, -2.7236e-09,  2.9732e-09,\n","         -2.2658e-09, -8.8622e-10, -5.4520e-09, -7.3312e-09, -2.9443e-09,\n","         -5.1664e-09,  1.2567e-09, -3.8414e-10, -6.3094e-10,  1.7284e-09,\n","         -6.8411e-09, -2.2759e-09,  1.3031e-09, -1.5967e-09,  1.6876e-09,\n","          1.2397e-10,  7.6822e-10, -4.7581e-09,  5.7223e-09, -4.1553e-10,\n","         -3.1588e-09,  9.5406e-10, -1.6517e-09,  4.9955e-09, -7.3052e-10,\n","          4.4820e-09,  1.3917e-09,  6.2483e-09, -4.4784e-09,  3.2951e-09,\n","         -1.9909e-09, -1.0668e-09,  6.2102e-10]], device='cuda:0')\n","\n","weight of a layer\n","\n","Parameter containing:\n","tensor([[ 0.0213, -0.0200,  0.0161,  ...,  0.0333,  0.0341,  0.0047],\n","        [-0.0401,  0.0288,  0.0134,  ...,  0.0038, -0.0283, -0.0025],\n","        [-0.0021,  0.0417, -0.0365,  ...,  0.0036,  0.0037, -0.0424],\n","        ...,\n","        [-0.0471, -0.0234,  0.0210,  ...,  0.0292,  0.0384,  0.0118],\n","        [-0.0301, -0.0059, -0.0146,  ...,  0.0164, -0.0310,  0.0148],\n","        [-0.0026, -0.0264, -0.0348,  ..., -0.0443,  0.0297, -0.0244]],\n","       device='cuda:0', requires_grad=True)\n","input tensor are:\n","torch.Size([1123, 1, 100])\n","torch.Size([7, 1, 100])\n","output are:\n","torch.Size([700])\n","torch.Size([700])\n","loss:\n","0.00019159670046065003\n","\n","\n","accu grad\n","\n","tensor([[-2.5804e-09, -2.3786e-09,  1.8962e-09,  3.5507e-09, -3.6838e-09,\n","          2.6425e-09,  1.4836e-09,  3.9747e-09,  8.4672e-10,  3.6829e-09,\n","          1.7445e-09,  3.7132e-10, -5.6312e-10, -4.4186e-10, -1.9790e-09,\n","         -3.8257e-09,  2.3509e-09,  5.9064e-09,  2.9064e-10, -3.5873e-10,\n","         -1.3285e-09,  2.6096e-09,  5.1072e-09,  5.6403e-09,  2.7396e-09,\n","          2.3946e-10,  5.1928e-09,  3.1851e-10, -4.0260e-09, -9.2340e-10,\n","         -2.4867e-09, -1.7285e-09, -1.3723e-09,  1.0876e-10, -4.4209e-09,\n","         -8.5778e-10, -1.2842e-09,  1.3951e-09, -2.7170e-11,  3.8539e-10,\n","          6.3346e-10, -3.9828e-09, -3.0214e-09,  6.9696e-09, -1.0494e-09,\n","         -1.5061e-09,  2.6545e-09,  3.2959e-09,  6.8173e-10,  1.1032e-09,\n","          4.3984e-09, -5.2527e-10,  4.7756e-09, -1.1195e-09,  1.7728e-09,\n","         -2.9505e-09, -3.5757e-09,  3.4550e-09,  2.1585e-09,  1.7634e-09,\n","         -3.2457e-09, -4.1641e-09, -5.0393e-09, -4.1241e-09, -2.0072e-10,\n","         -5.1535e-10,  2.2858e-09,  4.5704e-10,  1.2768e-09, -1.4055e-09,\n","          3.3451e-09,  2.0001e-09, -2.1450e-09,  2.6353e-09, -1.5796e-09,\n","          1.6452e-09,  2.6180e-09, -1.8056e-09,  1.6884e-09, -3.0168e-09,\n","         -8.8271e-11, -1.2318e-09, -2.6411e-09, -2.0501e-09,  2.8916e-09,\n","         -3.8679e-09,  1.9027e-09,  1.6384e-09,  1.0145e-08,  1.0248e-09,\n","          1.0558e-09, -5.3436e-10,  1.4866e-09, -2.3841e-09,  2.4439e-09,\n","         -1.7658e-09, -8.5121e-10, -4.7189e-09, -6.4281e-09, -2.8575e-09,\n","         -4.5393e-09,  1.3268e-09, -5.9875e-10, -2.9608e-10,  1.5743e-09,\n","         -6.2164e-09, -2.0468e-09,  1.1429e-09, -1.2675e-09,  1.3964e-09,\n","          3.1242e-10,  8.5957e-10, -4.5691e-09,  5.1340e-09, -4.1599e-10,\n","         -2.7105e-09,  1.0304e-09, -1.9616e-09,  4.3249e-09, -1.0172e-09,\n","          4.2214e-09,  1.5101e-09,  6.2377e-09, -4.2566e-09,  3.0750e-09,\n","         -1.8649e-09, -6.9044e-10,  9.5371e-10]], device='cuda:0')\n","\n","weight of a layer\n","\n","Parameter containing:\n","tensor([[ 0.0208, -0.0196,  0.0166,  ...,  0.0333,  0.0341,  0.0047],\n","        [-0.0395,  0.0283,  0.0127,  ...,  0.0038, -0.0283, -0.0025],\n","        [-0.0014,  0.0412, -0.0370,  ...,  0.0036,  0.0037, -0.0424],\n","        ...,\n","        [-0.0479, -0.0229,  0.0213,  ...,  0.0292,  0.0384,  0.0118],\n","        [-0.0308, -0.0055, -0.0145,  ...,  0.0164, -0.0310,  0.0148],\n","        [-0.0020, -0.0269, -0.0353,  ..., -0.0443,  0.0297, -0.0244]],\n","       device='cuda:0', requires_grad=True)\n","input tensor are:\n","torch.Size([1660, 1, 100])\n","torch.Size([12, 1, 100])\n","output are:\n","torch.Size([1200])\n","torch.Size([1200])\n","loss:\n","0.00018395116785541177\n","\n","\n","accu grad\n","\n","tensor([[-1.2712e-09, -1.2487e-09,  8.6349e-10,  1.7844e-09, -1.8909e-09,\n","          1.2565e-09,  6.5932e-10,  2.0250e-09,  1.9761e-10,  1.7431e-09,\n","          7.6634e-10,  2.2218e-10, -2.7571e-10, -2.7540e-10, -1.0191e-09,\n","         -1.8247e-09,  9.8784e-10,  2.9644e-09,  1.8516e-10, -2.2925e-10,\n","         -5.5736e-10,  1.4019e-09,  2.4901e-09,  2.6423e-09,  1.4950e-09,\n","          1.1714e-10,  2.6578e-09,  4.0062e-11, -1.8718e-09, -5.4094e-10,\n","         -1.2141e-09, -1.0155e-09, -6.9888e-10,  4.6500e-11, -2.1476e-09,\n","         -4.7157e-10, -6.9528e-10,  7.7757e-10,  5.6471e-11,  2.6461e-10,\n","          1.8820e-10, -2.1117e-09, -1.5215e-09,  3.4177e-09, -5.7046e-10,\n","         -7.9359e-10,  1.1973e-09,  1.7582e-09,  2.1895e-10,  6.3515e-10,\n","          2.1041e-09, -3.5968e-10,  2.2482e-09, -4.6764e-10,  1.0167e-09,\n","         -1.2803e-09, -1.7002e-09,  1.7464e-09,  1.0664e-09,  8.7834e-10,\n","         -1.6839e-09, -2.0806e-09, -2.5387e-09, -2.0177e-09,  5.6773e-11,\n","         -2.3690e-10,  1.1652e-09,  1.9308e-10,  5.3070e-10, -6.6589e-10,\n","          1.4192e-09,  1.0523e-09, -1.1054e-09,  1.2786e-09, -7.4692e-10,\n","          5.9717e-10,  1.2921e-09, -7.4545e-10,  6.7751e-10, -1.4757e-09,\n","          8.9956e-11, -6.2690e-10, -1.0634e-09, -1.0802e-09,  1.2975e-09,\n","         -1.8956e-09,  9.0224e-10,  7.0751e-10,  4.9180e-09,  6.1084e-10,\n","          6.8165e-10, -3.2031e-10,  8.3190e-10, -1.1481e-09,  1.0977e-09,\n","         -7.0724e-10, -4.0905e-10, -2.1908e-09, -2.9183e-09, -1.4039e-09,\n","         -2.1404e-09,  7.4281e-10, -3.8179e-10, -9.2431e-12,  7.6763e-10,\n","         -3.0562e-09, -9.6309e-10,  5.0527e-10, -5.8369e-10,  6.1056e-10,\n","          2.5749e-10,  3.9952e-10, -2.3366e-09,  2.4926e-09, -2.1659e-10,\n","         -1.2070e-09,  4.9456e-10, -1.2225e-09,  1.9170e-09, -6.1094e-10,\n","          2.1130e-09,  8.5323e-10,  3.3601e-09, -2.0604e-09,  1.4605e-09,\n","         -8.9197e-10, -2.0188e-10,  6.4382e-10]], device='cuda:0')\n","\n","weight of a layer\n","\n","Parameter containing:\n","tensor([[ 0.0204, -0.0193,  0.0170,  ...,  0.0333,  0.0341,  0.0047],\n","        [-0.0389,  0.0279,  0.0119,  ...,  0.0038, -0.0283, -0.0025],\n","        [-0.0006,  0.0408, -0.0375,  ...,  0.0036,  0.0037, -0.0424],\n","        ...,\n","        [-0.0487, -0.0224,  0.0216,  ...,  0.0292,  0.0384,  0.0118],\n","        [-0.0315, -0.0050, -0.0145,  ...,  0.0164, -0.0310,  0.0148],\n","        [-0.0013, -0.0273, -0.0359,  ..., -0.0443,  0.0297, -0.0244]],\n","       device='cuda:0', requires_grad=True)\n","input tensor are:\n","torch.Size([1833, 1, 100])\n","torch.Size([6, 1, 100])\n","output are:\n","torch.Size([600])\n","torch.Size([600])\n","loss:\n","0.00011204806651221588\n","\n","\n","accu grad\n","\n","tensor([[-3.4246e-10, -5.5061e-10,  2.9839e-10,  7.8508e-10, -8.6036e-10,\n","          4.3698e-10,  1.3729e-10,  8.3939e-10, -1.9838e-10,  6.6858e-10,\n","          2.0098e-10,  2.3426e-10, -2.9011e-10, -1.3008e-10, -3.0958e-10,\n","         -7.0052e-10,  2.2052e-10,  1.2219e-09,  4.5437e-11, -1.2420e-10,\n","         -1.7365e-11,  6.9588e-10,  8.7942e-10,  9.1882e-10,  6.0903e-10,\n","          7.3860e-11,  1.0123e-09, -1.4847e-10, -5.2585e-10, -2.9202e-10,\n","         -5.2438e-10, -6.4099e-10, -4.3407e-10, -1.3955e-11, -6.8935e-10,\n","         -1.8711e-10, -2.6567e-10,  2.4276e-10,  1.3454e-10,  2.2369e-10,\n","         -7.7421e-11, -8.9381e-10, -5.7908e-10,  1.2666e-09, -1.9061e-10,\n","         -3.3776e-10,  3.5516e-10,  8.9631e-10,  5.1758e-11,  3.9490e-10,\n","          7.9742e-10, -3.0859e-10,  7.6306e-10, -1.0142e-10,  5.3620e-10,\n","         -4.4755e-10, -5.6945e-10,  8.0839e-10,  4.3766e-10,  3.7046e-10,\n","         -6.2869e-10, -7.7109e-10, -1.1886e-09, -6.9315e-10,  2.4483e-10,\n","         -3.0830e-11,  5.0866e-10,  2.9657e-11,  9.4902e-11, -1.2039e-10,\n","          3.5985e-10,  3.6328e-10, -5.2725e-10,  3.1492e-10, -3.2808e-10,\n","          2.5571e-11,  4.5500e-10, -2.1284e-10,  1.3022e-10, -5.3832e-10,\n","          1.5120e-10, -2.0981e-10, -2.7031e-10, -4.6058e-10,  3.8794e-10,\n","         -6.6239e-10,  2.4846e-10,  2.1492e-10,  1.9919e-09,  2.8946e-10,\n","          3.7919e-10, -2.5004e-10,  4.3020e-10, -4.4971e-10,  4.0984e-10,\n","         -1.8715e-10, -5.3635e-11, -8.0816e-10, -9.5625e-10, -5.0382e-10,\n","         -8.4964e-10,  3.1526e-10, -1.7746e-10,  6.2166e-11,  2.9901e-10,\n","         -1.2975e-09, -3.6631e-10,  1.7026e-10, -2.7568e-10,  1.7694e-10,\n","          1.3966e-10, -5.1114e-11, -9.4304e-10,  9.8938e-10, -6.9625e-11,\n","         -4.4414e-10,  1.4687e-10, -7.3308e-10,  5.9594e-10, -2.7766e-10,\n","          8.1969e-10,  3.8382e-10,  1.7277e-09, -7.7186e-10,  4.3352e-10,\n","         -3.9352e-10, -2.5960e-11,  4.2116e-10]], device='cuda:0')\n","\n","weight of a layer\n","\n","Parameter containing:\n","tensor([[ 2.0292e-02, -1.9052e-02,  1.7312e-02,  ...,  3.3307e-02,\n","          3.4062e-02,  4.6874e-03],\n","        [-3.8147e-02,  2.7427e-02,  1.1199e-02,  ...,  3.8234e-03,\n","         -2.8328e-02, -2.5460e-03],\n","        [ 5.1144e-05,  4.0365e-02, -3.7971e-02,  ...,  3.6331e-03,\n","          3.7254e-03, -4.2414e-02],\n","        ...,\n","        [-4.9275e-02, -2.2043e-02,  2.1936e-02,  ...,  2.9221e-02,\n","          3.8435e-02,  1.1846e-02],\n","        [-3.2079e-02, -4.6535e-03, -1.4466e-02,  ...,  1.6407e-02,\n","         -3.1007e-02,  1.4773e-02],\n","        [-5.8884e-04, -2.7814e-02, -3.6472e-02,  ..., -4.4306e-02,\n","          2.9655e-02, -2.4423e-02]], device='cuda:0', requires_grad=True)\n","input tensor are:\n","torch.Size([917, 1, 100])\n","torch.Size([27, 1, 100])\n","output are:\n","torch.Size([2700])\n","torch.Size([2700])\n","loss:\n","0.00010968939022859558\n","\n","\n","accu grad\n","\n","tensor([[-2.6569e-10, -7.1478e-10,  3.0293e-10,  1.0061e-09, -1.1191e-09,\n","          4.6439e-10,  5.3418e-11,  1.0553e-09, -4.6931e-10,  7.5540e-10,\n","          1.2510e-10,  4.0637e-10, -4.7954e-10, -1.5955e-10, -2.6712e-10,\n","         -8.0460e-10,  1.1198e-10,  1.4976e-09,  2.7387e-11, -1.7186e-10,\n","          1.1609e-10,  9.0388e-10,  9.6156e-10,  9.8195e-10,  7.5524e-10,\n","          8.6875e-11,  1.1941e-09, -2.9562e-10, -4.8216e-10, -3.7429e-10,\n","         -6.6410e-10, -9.3048e-10, -6.0137e-10, -2.8636e-11, -7.0057e-10,\n","         -2.5359e-10, -2.9433e-10,  2.4223e-10,  2.5579e-10,  3.5547e-10,\n","         -2.1559e-10, -1.1078e-09, -6.6745e-10,  1.4419e-09, -2.0882e-10,\n","         -4.1150e-10,  3.2999e-10,  1.2202e-09,  5.5954e-11,  5.7151e-10,\n","          9.0802e-10, -4.7216e-10,  7.7831e-10, -6.2592e-11,  7.5091e-10,\n","         -4.5447e-10, -6.1530e-10,  1.0275e-09,  5.0325e-10,  4.5941e-10,\n","         -7.1945e-10, -8.7274e-10, -1.5430e-09, -7.0033e-10,  4.6751e-10,\n","          3.6683e-12,  5.9977e-10, -4.2070e-12,  3.3194e-11, -4.5007e-11,\n","          2.5525e-10,  3.9094e-10, -7.3378e-10,  2.4272e-10, -4.2313e-10,\n","         -1.5430e-10,  4.7440e-10, -1.6011e-10,  6.6910e-11, -6.1106e-10,\n","          2.8663e-10, -2.0197e-10, -1.7718e-10, -6.0033e-10,  3.3187e-10,\n","         -7.2019e-10,  2.0093e-10,  2.0787e-10,  2.3390e-09,  3.3874e-10,\n","          5.0933e-10, -3.9360e-10,  6.2075e-10, -5.2379e-10,  4.6928e-10,\n","         -1.5774e-10,  1.6520e-11, -9.0777e-10, -9.3506e-10, -5.5916e-10,\n","         -9.8979e-10,  4.2044e-10, -2.0032e-10,  1.1482e-10,  3.7779e-10,\n","         -1.6140e-09, -4.1849e-10,  1.5516e-10, -3.7953e-10,  1.7193e-10,\n","          1.9536e-10, -2.3441e-10, -1.1397e-09,  1.1844e-09, -9.4716e-11,\n","         -4.7730e-10,  1.2101e-10, -1.0350e-09,  5.7237e-10, -3.6129e-10,\n","          9.5313e-10,  4.8898e-10,  2.3221e-09, -8.2267e-10,  3.7424e-10,\n","         -4.7032e-10,  4.0486e-11,  6.1654e-10]], device='cuda:0')\n","\n","weight of a layer\n","\n","Parameter containing:\n","tensor([[ 0.0203, -0.0189,  0.0176,  ...,  0.0333,  0.0341,  0.0047],\n","        [-0.0374,  0.0270,  0.0105,  ...,  0.0038, -0.0283, -0.0025],\n","        [ 0.0006,  0.0400, -0.0384,  ...,  0.0036,  0.0037, -0.0424],\n","        ...,\n","        [-0.0495, -0.0218,  0.0222,  ...,  0.0292,  0.0384,  0.0118],\n","        [-0.0325, -0.0043, -0.0145,  ...,  0.0164, -0.0310,  0.0148],\n","        [ 0.0002, -0.0283, -0.0370,  ..., -0.0443,  0.0297, -0.0244]],\n","       device='cuda:0', requires_grad=True)\n","input tensor are:\n","torch.Size([963, 1, 100])\n","torch.Size([39, 1, 100])\n","output are:\n","torch.Size([3900])\n","torch.Size([3900])\n","loss:\n","7.883885700721294e-05\n","\n","\n","accu grad\n","\n","tensor([[ 9.1662e-11, -2.7868e-10,  6.7287e-11,  4.1152e-10, -4.4533e-10,\n","          8.8747e-11, -7.9159e-11,  3.7421e-10, -3.7746e-10,  2.3937e-10,\n","         -6.8203e-11,  2.7535e-10, -3.2959e-10, -5.0158e-11,  3.5456e-11,\n","         -2.4836e-10, -9.5524e-11,  5.0405e-10, -3.8349e-11, -7.9151e-11,\n","          2.1371e-10,  3.8265e-10,  2.1875e-10,  2.2518e-10,  2.3716e-10,\n","          6.1088e-11,  3.4315e-10, -2.1189e-10,  2.2250e-11, -1.8075e-10,\n","         -2.4288e-10, -4.7547e-10, -3.3643e-10, -3.3945e-11, -7.7389e-11,\n","         -9.9575e-11, -5.9123e-11, -4.3249e-13,  1.8366e-10,  2.2800e-10,\n","         -2.0294e-10, -3.6895e-10, -1.9496e-10,  3.9104e-10, -2.8246e-11,\n","         -1.5787e-10,  2.3156e-11,  5.8555e-10,  2.7440e-11,  3.0742e-10,\n","          2.6582e-10, -2.9529e-10,  1.3890e-10,  3.0892e-11,  3.5541e-10,\n","         -1.0375e-10, -1.1223e-10,  4.2748e-10,  1.7105e-10,  1.7089e-10,\n","         -1.7673e-10, -2.1039e-10, -6.8475e-10, -1.0667e-10,  3.3583e-10,\n","          4.2280e-11,  2.1664e-10, -4.6208e-11, -7.8623e-11,  1.1744e-10,\n","         -5.0872e-11,  4.2763e-11, -3.3551e-10, -1.1541e-10, -1.8222e-10,\n","         -2.0461e-10,  7.5978e-11,  1.4744e-11, -7.4188e-11, -1.5467e-10,\n","          1.7333e-10, -3.0642e-11,  5.0687e-11, -2.3791e-10,  5.6115e-12,\n","         -1.4726e-10, -4.9064e-11,  1.8909e-11,  8.0284e-10,  1.1335e-10,\n","          2.1867e-10, -2.4221e-10,  2.8332e-10, -1.7153e-10,  1.4159e-10,\n","         -8.0522e-12,  1.1919e-10, -2.7553e-10, -1.5273e-10, -1.4185e-10,\n","         -3.3186e-10,  1.5188e-10, -5.7749e-11,  6.7236e-11,  1.4369e-10,\n","         -6.3047e-10, -1.4966e-10,  1.5783e-11, -1.8812e-10,  6.6697e-12,\n","          6.7085e-11, -3.0463e-10, -3.6120e-10,  3.9136e-10, -2.0874e-11,\n","         -1.4556e-10,  2.6958e-12, -5.0400e-10,  8.7745e-11, -1.3839e-10,\n","          2.7515e-10,  1.8586e-10,  1.0828e-09, -2.2483e-10, -2.2608e-11,\n","         -2.1438e-10,  5.3958e-11,  3.3443e-10]], device='cuda:0')\n","\n","weight of a layer\n","\n","Parameter containing:\n","tensor([[ 0.0204, -0.0188,  0.0179,  ...,  0.0333,  0.0341,  0.0047],\n","        [-0.0366,  0.0265,  0.0099,  ...,  0.0038, -0.0283, -0.0025],\n","        [ 0.0009,  0.0398, -0.0388,  ...,  0.0036,  0.0037, -0.0424],\n","        ...,\n","        [-0.0494, -0.0218,  0.0225,  ...,  0.0292,  0.0384,  0.0118],\n","        [-0.0328, -0.0041, -0.0145,  ...,  0.0164, -0.0310,  0.0148],\n","        [ 0.0010, -0.0288, -0.0375,  ..., -0.0443,  0.0297, -0.0244]],\n","       device='cuda:0', requires_grad=True)\n","input tensor are:\n","torch.Size([1041, 1, 100])\n","torch.Size([19, 1, 100])\n","output are:\n","torch.Size([1900])\n","torch.Size([1900])\n","loss:\n","5.6473752920283005e-05\n","\n","\n","accu grad\n","\n","tensor([[ 1.3464e-10, -6.8108e-11, -7.2981e-12,  1.2145e-10, -1.1611e-10,\n","         -1.4758e-11, -6.4526e-11,  7.7409e-11, -2.0100e-10,  4.5631e-11,\n","         -8.3990e-11,  1.3444e-10, -1.6888e-10, -4.9178e-12,  9.6218e-11,\n","         -3.5991e-11, -9.5337e-11,  8.9944e-11, -4.3927e-11, -2.5173e-11,\n","          1.5506e-10,  1.1990e-10, -2.1930e-11, -1.4291e-11,  3.0765e-11,\n","          4.2315e-11,  2.3745e-11, -9.8911e-11,  1.1112e-10, -7.0151e-11,\n","         -4.9511e-11, -1.8648e-10, -1.4852e-10, -2.0027e-11,  9.1309e-11,\n","         -2.6262e-11,  2.2777e-11, -5.6275e-11,  9.3084e-11,  1.1245e-10,\n","         -1.2462e-10, -6.0160e-11, -1.0925e-11,  1.7006e-11,  3.0253e-11,\n","         -4.5174e-11, -4.7482e-11,  2.2017e-10,  1.4850e-11,  1.3246e-10,\n","          2.5933e-11, -1.4687e-10, -4.3689e-11,  3.4263e-11,  1.2273e-10,\n","          1.0187e-11,  4.2818e-11,  1.3083e-10,  2.7456e-11,  4.3512e-11,\n","          7.4937e-12,  1.5087e-11, -2.2828e-10,  6.4157e-11,  1.7029e-10,\n","          3.4884e-11,  5.4949e-11, -4.0807e-11, -7.4241e-11,  1.0997e-10,\n","         -8.1923e-11, -5.5153e-11, -1.0838e-10, -1.5865e-10, -5.7681e-11,\n","         -1.2112e-10, -3.7650e-11,  3.7540e-11, -7.2138e-11,  4.8071e-12,\n","          7.3689e-11,  9.9139e-12,  6.9899e-11, -6.2260e-11, -5.5551e-11,\n","          3.4964e-11, -8.6404e-11, -3.0421e-11,  1.6792e-10,  2.0625e-11,\n","          6.9022e-11, -1.1664e-10,  9.0199e-11, -3.3227e-11,  1.3329e-11,\n","          2.1560e-11,  9.2274e-11, -4.1359e-11,  6.0231e-11,  9.2785e-12,\n","         -5.7570e-11,  2.7105e-11, -5.3710e-12,  2.8310e-11,  3.7160e-11,\n","         -1.7481e-10, -4.1831e-11, -2.4349e-11, -7.5874e-11, -2.8006e-11,\n","          1.3112e-11, -2.0323e-10, -4.5070e-11,  6.0701e-11,  5.2731e-12,\n","         -2.0454e-11, -2.3223e-11, -1.8651e-10, -4.3670e-11, -2.9346e-11,\n","          1.7117e-11,  4.9938e-11,  3.8528e-10, -1.5896e-11, -9.1987e-11,\n","         -8.4985e-11,  3.0612e-11,  1.4265e-10]], device='cuda:0')\n","\n","weight of a layer\n","\n","Parameter containing:\n","tensor([[ 0.0205, -0.0187,  0.0182,  ...,  0.0333,  0.0341,  0.0047],\n","        [-0.0359,  0.0261,  0.0094,  ...,  0.0038, -0.0283, -0.0025],\n","        [ 0.0011,  0.0396, -0.0393,  ...,  0.0036,  0.0037, -0.0424],\n","        ...,\n","        [-0.0491, -0.0218,  0.0230,  ...,  0.0292,  0.0384,  0.0118],\n","        [-0.0330, -0.0039, -0.0145,  ...,  0.0164, -0.0310,  0.0148],\n","        [ 0.0017, -0.0292, -0.0378,  ..., -0.0443,  0.0297, -0.0244]],\n","       device='cuda:0', requires_grad=True)\n","input tensor are:\n","torch.Size([1122, 1, 100])\n","torch.Size([39, 1, 100])\n","output are:\n","torch.Size([3900])\n","torch.Size([3900])\n","loss:\n","4.700772842625156e-05\n","\n","\n","accu grad\n","\n","tensor([[ 1.2091e-11, -1.4292e-11,  3.5751e-12,  2.2385e-11, -2.1660e-11,\n","          2.1068e-11, -4.3395e-12,  2.1719e-11, -3.0073e-11,  1.5460e-11,\n","         -1.3596e-11,  1.7710e-11, -2.4323e-11, -1.8920e-12,  2.1340e-11,\n","         -1.5474e-11, -1.2121e-11,  3.4719e-11, -9.2462e-12,  7.3612e-13,\n","          1.8708e-11,  1.9854e-11,  1.4925e-11,  1.9715e-11,  1.8777e-11,\n","          9.4112e-12,  2.1720e-11, -1.1487e-11, -1.2836e-11, -3.6993e-12,\n","         -1.4932e-11, -3.2427e-11, -1.7343e-11,  1.8061e-12,  1.0077e-11,\n","         -2.8582e-12,  8.2866e-12, -6.6531e-12,  1.2849e-11,  1.3528e-11,\n","         -2.0109e-11, -2.3155e-11, -3.4231e-12,  2.4657e-11,  1.2241e-11,\n","         -4.6509e-12, -1.2912e-12,  3.3807e-11,  6.0397e-12,  2.0991e-11,\n","          1.2538e-11, -2.0024e-11,  6.4851e-12,  5.2834e-12,  1.1567e-11,\n","         -5.0247e-13, -5.9163e-12,  1.9244e-11,  1.8008e-13,  9.1249e-12,\n","         -9.6343e-12, -1.0579e-11, -3.4678e-11,  6.4112e-12,  2.2300e-11,\n","          9.4033e-12,  1.2211e-11, -7.3513e-12, -7.2894e-12,  1.1479e-11,\n","         -2.6811e-12, -3.3864e-12, -1.2771e-11, -1.1881e-11, -7.6324e-12,\n","         -1.3456e-11,  1.6908e-12,  2.8920e-12,  4.6857e-12, -7.1783e-12,\n","          9.9277e-12, -3.0506e-12, -5.0479e-13, -9.7281e-12,  7.5898e-13,\n","         -1.3378e-12, -5.8854e-12,  3.4639e-12,  4.2621e-11,  1.5497e-12,\n","          1.2184e-11, -1.1601e-11,  1.5849e-11, -1.0161e-11,  9.5557e-12,\n","         -2.9848e-12,  3.3521e-12, -1.8896e-11, -3.1131e-12,  3.5819e-12,\n","         -6.4447e-12,  6.4966e-12,  5.5792e-13,  4.7884e-12,  1.2295e-11,\n","         -3.5115e-11, -1.4371e-11, -7.3412e-12, -1.3555e-11,  1.0631e-11,\n","          7.7817e-12, -1.7873e-11, -1.2242e-11,  1.4440e-11, -9.4407e-12,\n","         -3.5597e-12, -1.2258e-11, -2.5936e-11, -8.6274e-12, -1.9439e-12,\n","          7.4787e-12,  9.5937e-12,  5.1431e-11,  8.9634e-13, -8.1270e-12,\n","         -1.1668e-11,  6.1906e-12,  1.9750e-11]], device='cuda:0')\n","\n","weight of a layer\n","\n","Parameter containing:\n","tensor([[ 0.0206, -0.0186,  0.0184,  ...,  0.0333,  0.0341,  0.0047],\n","        [-0.0352,  0.0257,  0.0090,  ...,  0.0038, -0.0283, -0.0025],\n","        [ 0.0011,  0.0395, -0.0399,  ...,  0.0036,  0.0037, -0.0424],\n","        ...,\n","        [-0.0488, -0.0219,  0.0235,  ...,  0.0292,  0.0384,  0.0118],\n","        [-0.0333, -0.0037, -0.0145,  ...,  0.0164, -0.0310,  0.0148],\n","        [ 0.0024, -0.0296, -0.0381,  ..., -0.0443,  0.0297, -0.0244]],\n","       device='cuda:0', requires_grad=True)\n","input tensor are:\n","torch.Size([1030, 1, 100])\n","torch.Size([10, 1, 100])\n","output are:\n","torch.Size([1000])\n","torch.Size([1000])\n","loss:\n","4.526070551946759e-05\n","\n","\n","accu grad\n","\n","tensor([[-3.0555e-11, -5.1989e-11,  2.0266e-11,  4.9484e-11, -7.8184e-11,\n","          8.6192e-11,  1.3163e-11,  8.9798e-11, -2.0768e-11,  5.1238e-11,\n","          1.0922e-11,  1.4982e-11, -2.6013e-11, -1.2304e-11,  5.7973e-12,\n","         -6.6634e-11,  1.3838e-11,  1.3200e-10, -5.0361e-12,  4.6627e-12,\n","         -1.5528e-11,  4.2258e-11,  9.4945e-11,  1.0035e-10,  7.1768e-11,\n","          1.2569e-11,  1.0763e-10, -7.0827e-12, -9.4491e-11,  1.5904e-11,\n","         -6.0952e-11, -5.6852e-11, -4.9650e-12,  1.0698e-11, -4.2804e-11,\n","         -1.5046e-11, -6.0929e-12,  1.6745e-11,  3.9129e-12, -1.7421e-12,\n","         -1.1453e-11, -9.7195e-11, -4.2743e-11,  1.2329e-10,  5.6634e-12,\n","         -9.2967e-12,  2.5152e-11,  5.0170e-11,  8.4137e-12,  2.7437e-11,\n","          6.4843e-11, -1.9544e-11,  6.7182e-11, -5.6182e-13,  1.6288e-11,\n","         -2.7384e-11, -7.4147e-11,  4.6124e-11,  1.2999e-11,  2.8632e-11,\n","         -6.1886e-11, -7.7161e-11, -6.5381e-11, -3.6932e-11,  1.6424e-11,\n","          1.6309e-11,  4.0763e-11, -1.1838e-12,  1.5916e-11, -2.3715e-11,\n","          3.3270e-11,  3.5755e-11, -2.3515e-11,  5.9362e-11, -1.0973e-11,\n","         -1.8260e-12,  4.2409e-11, -1.4539e-11,  5.1747e-11, -5.0089e-11,\n","          1.4427e-11, -1.6482e-11, -3.9375e-11, -3.4979e-11,  3.2844e-11,\n","         -5.9045e-11,  3.4301e-11,  3.6764e-11,  1.5286e-10,  1.4058e-11,\n","          2.8512e-11, -5.9019e-13,  4.8043e-11, -2.8707e-11,  4.4758e-11,\n","         -2.8625e-11, -3.0996e-11, -7.4109e-11, -8.0490e-11, -2.4128e-11,\n","         -4.6790e-11,  3.0483e-11, -2.0964e-12,  8.3912e-12,  3.6808e-11,\n","         -9.9056e-11, -4.0731e-11, -1.7014e-12, -2.5345e-11,  4.9182e-11,\n","          2.7631e-11,  3.0873e-11, -7.5576e-11,  7.2975e-11, -3.7423e-11,\n","         -1.7477e-11, -2.3734e-11, -3.4822e-11,  2.6595e-11, -1.2288e-11,\n","          6.9266e-11,  1.5463e-11,  7.8578e-11, -5.2975e-12,  2.9710e-11,\n","         -1.0752e-11,  1.6401e-12,  1.5003e-11]], device='cuda:0')\n","\n","weight of a layer\n","\n","Parameter containing:\n","tensor([[ 0.0206, -0.0185,  0.0185,  ...,  0.0333,  0.0341,  0.0047],\n","        [-0.0346,  0.0254,  0.0085,  ...,  0.0038, -0.0283, -0.0025],\n","        [ 0.0010,  0.0394, -0.0406,  ...,  0.0036,  0.0037, -0.0424],\n","        ...,\n","        [-0.0485, -0.0220,  0.0241,  ...,  0.0292,  0.0384,  0.0118],\n","        [-0.0336, -0.0035, -0.0148,  ...,  0.0164, -0.0310,  0.0148],\n","        [ 0.0030, -0.0299, -0.0385,  ..., -0.0443,  0.0297, -0.0244]],\n","       device='cuda:0', requires_grad=True)\n","input tensor are:\n","torch.Size([1665, 1, 100])\n","torch.Size([12, 1, 100])\n","output are:\n","torch.Size([1200])\n","torch.Size([1200])\n","loss:\n","4.458384864847176e-05\n","\n","\n","accu grad\n","\n","tensor([[-6.2202e-11, -6.2292e-11,  2.7660e-11,  4.5980e-11, -9.0265e-11,\n","          9.1803e-11,  1.8621e-11,  1.0443e-10,  4.2053e-12,  5.3280e-11,\n","          3.3445e-11,  2.2894e-12, -3.9558e-12, -1.6655e-11, -2.6542e-11,\n","         -7.8340e-11,  3.2256e-11,  1.4757e-10,  8.2416e-12,  4.5999e-12,\n","         -4.8348e-11,  3.6139e-11,  1.2059e-10,  1.1890e-10,  8.8041e-11,\n","          2.2503e-12,  1.3598e-10,  2.5338e-13, -1.1748e-10,  2.3349e-11,\n","         -7.0372e-11, -3.9683e-11,  9.1494e-12,  1.1798e-11, -8.2552e-11,\n","         -1.9107e-11, -2.3172e-11,  3.6738e-11, -6.5634e-12, -1.6379e-11,\n","          1.2719e-11, -1.1399e-10, -6.8479e-11,  1.4855e-10, -1.2053e-11,\n","         -8.3831e-12,  4.1259e-11,  3.4016e-11,  4.0231e-12,  1.3290e-11,\n","          7.7456e-11, -5.4192e-13,  8.7027e-11, -4.1767e-12,  1.5564e-11,\n","         -3.8696e-11, -1.0125e-10,  4.3462e-11,  2.6493e-11,  2.5365e-11,\n","         -7.4712e-11, -9.8134e-11, -6.0233e-11, -6.7098e-11, -1.1897e-12,\n","          8.6318e-12,  4.3971e-11,  1.0891e-11,  3.6937e-11, -5.1265e-11,\n","          4.9681e-11,  5.7740e-11, -2.6308e-11,  1.1041e-10, -9.6469e-12,\n","          9.8707e-12,  5.6518e-11, -1.8459e-11,  6.9857e-11, -6.4018e-11,\n","          5.7322e-12, -1.7953e-11, -5.6389e-11, -4.3787e-11,  4.4055e-11,\n","         -8.3109e-11,  6.2942e-11,  5.2532e-11,  1.7410e-10,  2.0480e-11,\n","          2.5657e-11,  1.6688e-11,  5.1174e-11, -2.7756e-11,  5.6883e-11,\n","         -3.9058e-11, -4.3592e-11, -8.3748e-11, -1.0819e-10, -4.7646e-11,\n","         -6.0189e-11,  4.0052e-11, -3.9113e-12,  1.0768e-11,  4.3808e-11,\n","         -1.0126e-10, -3.9539e-11,  1.1508e-11, -1.7197e-11,  5.7403e-11,\n","          2.5936e-11,  6.8429e-11, -9.6780e-11,  9.3205e-11, -4.2931e-11,\n","         -2.2752e-11, -1.6311e-11, -1.9172e-11,  4.7965e-11, -2.4280e-11,\n","          9.5287e-11,  1.0744e-11,  5.8924e-11, -1.3093e-11,  4.7912e-11,\n","          1.9767e-12, -3.9044e-12,  2.2523e-12]], device='cuda:0')\n","\n","weight of a layer\n","\n","Parameter containing:\n","tensor([[ 0.0206, -0.0183,  0.0184,  ...,  0.0333,  0.0341,  0.0047],\n","        [-0.0341,  0.0251,  0.0079,  ...,  0.0038, -0.0283, -0.0025],\n","        [ 0.0009,  0.0393, -0.0413,  ...,  0.0036,  0.0037, -0.0424],\n","        ...,\n","        [-0.0482, -0.0220,  0.0245,  ...,  0.0292,  0.0384,  0.0118],\n","        [-0.0339, -0.0033, -0.0152,  ...,  0.0164, -0.0310,  0.0148],\n","        [ 0.0035, -0.0302, -0.0390,  ..., -0.0443,  0.0297, -0.0244]],\n","       device='cuda:0', requires_grad=True)\n","input tensor are:\n","torch.Size([1065, 1, 100])\n","torch.Size([16, 1, 100])\n","output are:\n","torch.Size([1600])\n","torch.Size([1600])\n","loss:\n","4.535353582468815e-05\n","\n","\n","accu grad\n","\n","tensor([[-1.1633e-10, -1.5250e-10,  4.9320e-11,  1.0892e-10, -2.1288e-10,\n","          1.7998e-10,  3.0893e-11,  2.3783e-10, -1.4789e-11,  1.1594e-10,\n","          6.7681e-11,  2.6228e-11, -2.1218e-11, -3.9814e-11, -5.7527e-11,\n","         -1.6845e-10,  5.8612e-11,  3.1263e-10,  1.4389e-11,  2.4626e-12,\n","         -9.3157e-11,  8.1621e-11,  2.5190e-10,  2.4417e-10,  1.9699e-10,\n","          1.0702e-11,  2.9660e-10, -7.2095e-12, -2.2230e-10,  3.9992e-11,\n","         -1.5313e-10, -9.1447e-11,  4.0639e-12,  1.9681e-11, -1.8529e-10,\n","         -5.4943e-11, -5.2950e-11,  7.4570e-11, -7.4444e-13, -2.4296e-11,\n","          2.0361e-11, -2.4940e-10, -1.7062e-10,  3.1366e-10, -3.8368e-11,\n","         -2.7153e-11,  8.9018e-11,  9.5915e-11,  5.8420e-12,  3.9905e-11,\n","          1.6516e-10, -1.2957e-11,  1.7489e-10, -6.1144e-12,  5.3586e-11,\n","         -7.2883e-11, -2.2046e-10,  1.0447e-10,  7.6853e-11,  5.3905e-11,\n","         -1.5619e-10, -2.0436e-10, -1.6047e-10, -1.4137e-10,  1.3297e-11,\n","          1.3287e-11,  9.9366e-11,  2.5743e-11,  7.8026e-11, -1.0786e-10,\n","          9.5510e-11,  1.1580e-10, -7.9667e-11,  2.3473e-10, -2.7138e-11,\n","          1.8375e-12,  1.0907e-10, -2.2408e-11,  1.4654e-10, -1.3459e-10,\n","          1.3178e-11, -3.6648e-11, -1.0883e-10, -1.1399e-10,  7.6924e-11,\n","         -1.7886e-10,  1.3053e-10,  1.0904e-10,  3.8986e-10,  5.2510e-11,\n","          5.3409e-11,  2.4768e-11,  1.2554e-10, -5.4607e-11,  1.2410e-10,\n","         -8.7077e-11, -7.3620e-11, -1.8350e-10, -2.2351e-10, -1.1441e-10,\n","         -1.3289e-10,  9.6775e-11, -1.2381e-11,  3.5034e-11,  1.0600e-10,\n","         -2.3902e-10, -9.3068e-11,  2.4498e-11, -3.7615e-11,  1.1388e-10,\n","          4.7312e-11,  1.2566e-10, -2.1602e-10,  2.0916e-10, -8.5559e-11,\n","         -4.3549e-11, -3.7651e-11, -4.8021e-11,  9.9917e-11, -7.1336e-11,\n","          2.1999e-10,  2.1652e-11,  1.6697e-10, -2.1990e-11,  8.4554e-11,\n","          1.3301e-12, -1.3146e-12,  2.4173e-11]], device='cuda:0')\n","\n","weight of a layer\n","\n","Parameter containing:\n","tensor([[ 0.0205, -0.0182,  0.0181,  ...,  0.0333,  0.0341,  0.0047],\n","        [-0.0337,  0.0249,  0.0073,  ...,  0.0038, -0.0283, -0.0025],\n","        [ 0.0008,  0.0393, -0.0420,  ...,  0.0036,  0.0037, -0.0424],\n","        ...,\n","        [-0.0481, -0.0220,  0.0244,  ...,  0.0292,  0.0384,  0.0118],\n","        [-0.0342, -0.0031, -0.0157,  ...,  0.0164, -0.0310,  0.0148],\n","        [ 0.0038, -0.0304, -0.0396,  ..., -0.0443,  0.0297, -0.0244]],\n","       device='cuda:0', requires_grad=True)\n","input tensor are:\n","torch.Size([961, 1, 100])\n","torch.Size([6, 1, 100])\n","output are:\n","torch.Size([600])\n","torch.Size([600])\n","loss:\n","4.404284118209034e-05\n","\n","\n","accu grad\n","\n","tensor([[-5.2306e-11, -1.9549e-10,  2.3066e-11,  1.5940e-10, -2.9229e-10,\n","          1.9420e-10,  1.0781e-11,  3.0419e-10, -1.0204e-10,  1.4619e-10,\n","          3.8892e-11,  9.7232e-11, -9.8798e-11, -4.5426e-11, -2.6085e-12,\n","         -1.9838e-10,  2.9546e-11,  3.5706e-10, -1.2185e-11, -1.4877e-11,\n","         -2.9291e-11,  1.2829e-10,  2.5822e-10,  2.4592e-10,  2.3475e-10,\n","          5.4223e-11,  3.3106e-10, -3.8135e-11, -1.7901e-10,  2.2081e-11,\n","         -1.7522e-10, -1.6138e-10, -5.0469e-11,  1.4878e-11, -1.6594e-10,\n","         -8.9038e-11, -4.1188e-11,  5.1515e-11,  4.4915e-11,  2.1951e-11,\n","         -4.1750e-11, -2.9403e-10, -1.9730e-10,  3.3653e-10, -2.8491e-11,\n","         -5.0189e-11,  8.2787e-11,  1.9979e-10,  1.2119e-11,  9.7143e-11,\n","          1.7010e-10, -9.5867e-11,  1.6374e-10,  1.2592e-11,  1.0360e-10,\n","         -5.2222e-11, -2.2990e-10,  1.7007e-10,  1.0926e-10,  6.4128e-11,\n","         -1.6258e-10, -2.0367e-10, -2.5673e-10, -1.1354e-10,  9.0327e-11,\n","          3.3771e-11,  1.4212e-10,  5.4519e-12,  5.9727e-11, -7.8387e-11,\n","          5.9574e-11,  9.2500e-11, -1.3791e-10,  1.9658e-10, -4.9597e-11,\n","         -5.3250e-11,  8.7099e-11,  1.0841e-13,  1.4548e-10, -1.3317e-10,\n","          2.9343e-11, -3.5688e-11, -9.3507e-11, -1.5775e-10,  4.5751e-11,\n","         -1.8370e-10,  1.1286e-10,  1.0202e-10,  4.9217e-10,  7.4171e-11,\n","          7.3795e-11, -2.6140e-11,  1.7706e-10, -5.4687e-11,  1.4442e-10,\n","         -9.8309e-11, -2.7923e-11, -2.1574e-10, -2.3111e-10, -1.2214e-10,\n","         -1.4962e-10,  1.1708e-10, -1.6062e-11,  6.8915e-11,  1.4700e-10,\n","         -3.3214e-10, -1.3097e-10,  4.5849e-12, -6.1255e-11,  1.1483e-10,\n","          5.2510e-11,  4.6805e-11, -2.4951e-10,  2.3835e-10, -9.2213e-11,\n","         -4.2709e-11, -7.0294e-11, -1.0829e-10,  9.0217e-11, -1.0136e-10,\n","          2.5734e-10,  2.6831e-11,  3.1901e-10, -1.5902e-12,  3.3206e-11,\n","         -3.1815e-11,  1.0995e-11,  9.5985e-11]], device='cuda:0')\n","\n","weight of a layer\n","\n","Parameter containing:\n","tensor([[ 0.0204, -0.0181,  0.0176,  ...,  0.0333,  0.0341,  0.0047],\n","        [-0.0334,  0.0247,  0.0065,  ...,  0.0038, -0.0283, -0.0025],\n","        [ 0.0007,  0.0392, -0.0425,  ...,  0.0036,  0.0037, -0.0424],\n","        ...,\n","        [-0.0480, -0.0220,  0.0240,  ...,  0.0292,  0.0384,  0.0118],\n","        [-0.0345, -0.0029, -0.0163,  ...,  0.0164, -0.0310,  0.0148],\n","        [ 0.0041, -0.0306, -0.0404,  ..., -0.0443,  0.0297, -0.0244]],\n","       device='cuda:0', requires_grad=True)\n","input tensor are:\n","torch.Size([1393, 1, 100])\n","torch.Size([7, 1, 100])\n","output are:\n","torch.Size([700])\n","torch.Size([700])\n","loss:\n","4.2603100155247375e-05\n","\n","\n","accu grad\n","\n","tensor([[-3.3613e-11, -1.5192e-10,  7.4399e-12,  1.2027e-10, -2.3263e-10,\n","          1.4133e-10,  3.7956e-12,  2.3618e-10, -8.5523e-11,  1.1293e-10,\n","          2.9528e-11,  8.3418e-11, -7.7613e-11, -3.5427e-11, -1.3131e-12,\n","         -1.5160e-10,  1.9899e-11,  2.6223e-10, -7.6757e-12, -1.3839e-11,\n","         -1.4979e-11,  9.8702e-11,  1.9168e-10,  1.7667e-10,  1.8512e-10,\n","          5.0991e-11,  2.5207e-10, -3.1210e-11, -1.1728e-10,  9.9622e-12,\n","         -1.3019e-10, -1.2061e-10, -4.6021e-11,  9.2859e-12, -1.3433e-10,\n","         -7.3099e-11, -3.2285e-11,  3.6733e-11,  4.1480e-11,  2.1979e-11,\n","         -3.6549e-11, -2.2353e-10, -1.6132e-10,  2.4721e-10, -2.5124e-11,\n","         -4.1048e-11,  6.5493e-11,  1.6369e-10,  5.0209e-12,  7.8681e-11,\n","          1.1915e-10, -8.6345e-11,  1.2170e-10,  1.6455e-11,  8.6070e-11,\n","         -3.2494e-11, -1.7670e-10,  1.3511e-10,  9.8587e-11,  4.0777e-11,\n","         -1.1846e-10, -1.4674e-10, -2.0499e-10, -8.5131e-11,  7.9171e-11,\n","          2.5530e-11,  1.1605e-10,  2.5101e-12,  4.9608e-11, -6.0349e-11,\n","          3.7213e-11,  6.4903e-11, -1.1492e-10,  1.5047e-10, -3.8834e-11,\n","         -4.7824e-11,  5.7774e-11,  8.2823e-12,  1.1168e-10, -9.5513e-11,\n","          1.6049e-11, -2.8889e-11, -6.9222e-11, -1.2692e-10,  2.4937e-11,\n","         -1.3607e-10,  9.0061e-11,  7.6235e-11,  3.8258e-10,  6.6074e-11,\n","          5.4458e-11, -2.8211e-11,  1.3761e-10, -3.5781e-11,  1.1169e-10,\n","         -7.7240e-11, -7.3176e-12, -1.6238e-10, -1.7555e-10, -9.9576e-11,\n","         -1.1113e-10,  9.0918e-11, -1.6646e-11,  6.4316e-11,  1.1771e-10,\n","         -2.5745e-10, -9.9832e-11,  3.3749e-12, -4.1560e-11,  8.4938e-11,\n","          3.4939e-11,  2.6309e-11, -1.9183e-10,  1.8189e-10, -6.7263e-11,\n","         -3.2650e-11, -5.7408e-11, -8.4336e-11,  6.8438e-11, -8.9649e-11,\n","          2.0496e-10,  1.8929e-11,  2.5872e-10,  2.2204e-12,  9.9653e-12,\n","         -2.3859e-11,  1.0665e-11,  8.5700e-11]], device='cuda:0')\n","\n","weight of a layer\n","\n","Parameter containing:\n","tensor([[ 0.0203, -0.0180,  0.0170,  ...,  0.0333,  0.0341,  0.0047],\n","        [-0.0332,  0.0246,  0.0056,  ...,  0.0038, -0.0283, -0.0025],\n","        [ 0.0007,  0.0392, -0.0428,  ...,  0.0036,  0.0037, -0.0424],\n","        ...,\n","        [-0.0480, -0.0220,  0.0235,  ...,  0.0292,  0.0384,  0.0118],\n","        [-0.0348, -0.0028, -0.0167,  ...,  0.0164, -0.0310,  0.0148],\n","        [ 0.0043, -0.0308, -0.0413,  ..., -0.0443,  0.0297, -0.0244]],\n","       device='cuda:0', requires_grad=True)\n","input tensor are:\n","torch.Size([1024, 1, 100])\n","torch.Size([10, 1, 100])\n","output are:\n","torch.Size([1000])\n","torch.Size([1000])\n","loss:\n","4.2271036363672465e-05\n","\n","\n","accu grad\n","\n","tensor([[-4.9882e-11, -2.2665e-10, -2.1341e-12,  1.7158e-10, -3.4822e-10,\n","          1.9636e-10,  4.2908e-12,  3.4740e-10, -1.2794e-10,  1.6737e-10,\n","          4.6700e-11,  1.2955e-10, -1.0717e-10, -5.2457e-11, -9.0410e-12,\n","         -2.2007e-10,  2.7719e-11,  3.6942e-10, -5.7387e-12, -2.0422e-11,\n","         -1.9172e-11,  1.4187e-10,  2.7536e-10,  2.4747e-10,  2.7610e-10,\n","          8.4843e-11,  3.6647e-10, -4.5064e-11, -1.5055e-10,  3.6046e-12,\n","         -1.8645e-10, -1.6740e-10, -7.1163e-11,  1.0971e-11, -2.1747e-10,\n","         -1.1202e-10, -4.8802e-11,  5.1720e-11,  6.6084e-11,  3.2837e-11,\n","         -5.3188e-11, -3.2496e-10, -2.5362e-10,  3.5036e-10, -4.6294e-11,\n","         -6.3473e-11,  1.0177e-10,  2.4843e-10, -9.8239e-13,  1.1634e-10,\n","          1.6442e-10, -1.3646e-10,  1.8054e-10,  2.9525e-11,  1.3297e-10,\n","         -3.9552e-11, -2.6616e-10,  1.9956e-10,  1.6836e-10,  5.1476e-11,\n","         -1.7035e-10, -2.0493e-10, -3.0949e-10, -1.2942e-10,  1.1991e-10,\n","          3.1726e-11,  1.7724e-10,  3.7032e-12,  8.1365e-11, -9.5076e-11,\n","          4.8419e-11,  9.0366e-11, -1.7711e-10,  2.2472e-10, -5.5105e-11,\n","         -7.3421e-11,  7.7159e-11,  2.4112e-11,  1.6701e-10, -1.3650e-10,\n","          1.4593e-11, -4.9154e-11, -9.8172e-11, -1.9410e-10,  2.5008e-11,\n","         -1.9468e-10,  1.3764e-10,  1.0906e-10,  5.6394e-10,  1.0971e-10,\n","          7.4422e-11, -4.9169e-11,  2.0362e-10, -4.7748e-11,  1.6351e-10,\n","         -1.1462e-10,  2.7936e-12, -2.3475e-10, -2.5891e-10, -1.5650e-10,\n","         -1.6004e-10,  1.3654e-10, -3.5015e-11,  1.0646e-10,  1.7524e-10,\n","         -3.8021e-10, -1.4407e-10,  6.1620e-12, -5.1648e-11,  1.2198e-10,\n","          4.4138e-11,  3.3633e-11, -2.8425e-10,  2.6839e-10, -9.1450e-11,\n","         -4.8839e-11, -8.7764e-11, -1.2018e-10,  1.0156e-10, -1.4616e-10,\n","          3.1452e-10,  2.9034e-11,  3.9198e-10,  4.7767e-12, -4.0764e-14,\n","         -3.1932e-11,  2.2767e-11,  1.3663e-10]], device='cuda:0')\n","\n","weight of a layer\n","\n","Parameter containing:\n","tensor([[ 0.0202, -0.0178,  0.0163,  ...,  0.0333,  0.0341,  0.0047],\n","        [-0.0330,  0.0245,  0.0047,  ...,  0.0038, -0.0283, -0.0025],\n","        [ 0.0007,  0.0391, -0.0429,  ...,  0.0036,  0.0037, -0.0424],\n","        ...,\n","        [-0.0481, -0.0219,  0.0228,  ...,  0.0292,  0.0384,  0.0118],\n","        [-0.0350, -0.0027, -0.0170,  ...,  0.0164, -0.0310,  0.0148],\n","        [ 0.0045, -0.0309, -0.0422,  ..., -0.0443,  0.0297, -0.0244]],\n","       device='cuda:0', requires_grad=True)\n","input tensor are:\n","torch.Size([1039, 1, 100])\n","torch.Size([6, 1, 100])\n","output are:\n","torch.Size([600])\n","torch.Size([600])\n","loss:\n","3.248591019655578e-05\n","\n","\n","accu grad\n","\n","tensor([[-1.3140e-11, -1.7469e-10, -2.0468e-11,  1.3721e-10, -2.7706e-10,\n","          1.4537e-10, -4.1264e-12,  2.6784e-10, -1.2393e-10,  1.3259e-10,\n","          2.2695e-11,  1.2316e-10, -1.0112e-10, -3.7065e-11,  1.7662e-11,\n","         -1.6340e-10,  7.3689e-12,  2.7029e-10, -1.0456e-11, -1.8119e-11,\n","          1.6527e-11,  1.1728e-10,  1.9487e-10,  1.7202e-10,  2.0763e-10,\n","          8.5628e-11,  2.6453e-10, -4.2332e-11, -8.4223e-11, -9.8374e-12,\n","         -1.3741e-10, -1.4042e-10, -6.9935e-11,  7.4871e-12, -1.5430e-10,\n","         -9.1117e-11, -2.6263e-11,  2.3289e-11,  6.6785e-11,  4.0566e-11,\n","         -6.4644e-11, -2.4467e-10, -1.8878e-10,  2.4830e-10, -2.5500e-11,\n","         -5.2534e-11,  7.4686e-11,  2.2148e-10, -3.2926e-12,  1.0143e-10,\n","          1.1111e-10, -1.3769e-10,  1.3173e-10,  3.2744e-11,  1.1181e-10,\n","         -1.4814e-11, -1.9783e-10,  1.6691e-10,  1.4433e-10,  3.5648e-11,\n","         -1.2344e-10, -1.3802e-10, -2.5579e-10, -8.3392e-11,  1.1599e-10,\n","          3.0837e-11,  1.4739e-10, -8.9128e-12,  5.3514e-11, -5.9605e-11,\n","          1.5327e-11,  5.0477e-11, -1.4837e-10,  1.4043e-10, -4.4720e-11,\n","         -7.0270e-11,  4.3091e-11,  2.9193e-11,  1.2224e-10, -9.2750e-11,\n","          1.0392e-11, -4.2339e-11, -6.2567e-11, -1.5203e-10, -6.2696e-13,\n","         -1.3490e-10,  9.1030e-11,  7.2863e-11,  4.3695e-10,  9.1045e-11,\n","          5.7135e-11, -6.1824e-11,  1.6096e-10, -3.4241e-11,  1.2432e-10,\n","         -8.2871e-11,  2.5451e-11, -1.7564e-10, -1.9146e-10, -1.1437e-10,\n","         -1.1345e-10,  1.0379e-10, -3.3875e-11,  9.3565e-11,  1.3837e-10,\n","         -3.0306e-10, -1.1386e-10, -5.2151e-12, -3.8248e-11,  8.9235e-11,\n","          3.1133e-11, -1.0007e-11, -2.1463e-10,  1.9797e-10, -6.4567e-11,\n","         -3.8265e-11, -8.1578e-11, -1.0942e-10,  6.6254e-11, -1.1700e-10,\n","          2.4051e-10,  2.6741e-11,  3.3752e-10,  1.1509e-11, -2.8918e-11,\n","         -3.6373e-11,  2.7490e-11,  1.3011e-10]], device='cuda:0')\n","\n","weight of a layer\n","\n","Parameter containing:\n","tensor([[ 0.0201, -0.0177,  0.0156,  ...,  0.0333,  0.0341,  0.0047],\n","        [-0.0329,  0.0245,  0.0037,  ...,  0.0038, -0.0283, -0.0025],\n","        [ 0.0007,  0.0390, -0.0427,  ...,  0.0036,  0.0037, -0.0424],\n","        ...,\n","        [-0.0482, -0.0218,  0.0220,  ...,  0.0292,  0.0384,  0.0118],\n","        [-0.0352, -0.0027, -0.0170,  ...,  0.0164, -0.0310,  0.0148],\n","        [ 0.0045, -0.0310, -0.0432,  ..., -0.0443,  0.0297, -0.0244]],\n","       device='cuda:0', requires_grad=True)\n","input tensor are:\n","torch.Size([1037, 1, 100])\n","torch.Size([31, 1, 100])\n","output are:\n","torch.Size([3100])\n","torch.Size([3100])\n","loss:\n","3.574067523004487e-05\n","\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-51-d09cd51eb473>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlossfunction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimiser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-50-6d7935d985ae>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(grid, X_train, y_train, model, lossfunction, optimiser)\u001b[0m\n\u001b[1;32m     39\u001b[0m       \u001b[0mtrainloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m       \u001b[0moptimiser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"BdwDHexBo8Qw","colab_type":"code","outputId":"50e31dbb-a9ab-4f46-9ad6-6fbe5b82c11e","executionInfo":{"status":"error","timestamp":1582934357425,"user_tz":0,"elapsed":5126,"user":{"displayName":"Michael River","photoUrl":"","userId":"13207336642017563139"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["def training(model, X_train,y_train, optimiser, lossfunction, clip):\n","  model.train()\n","  epoch_loss = 0\n","\n","  local_batch, local_labels = X_train.unsqueeze(1).to(device), y_train.unsqueeze(1).to(device) \n","  local_labels = local_labels.unsqueeze(2)\n","\n","  # print('input')\n","  # print(local_batch.size())\n","  # print(local_labels.size())\n","\n","  ### forward pass\n","  optimiser.zero_grad()\n","  \n","  local_output = model(seq2seq_input=local_batch, target=local_labels, teacher_forcing_ratio = 1)\n","  local_output = local_output.view(-1)[1:]\n","  local_labels = local_labels.view(-1)[1:]\n","\n","  # print('output:')\n","  # print(local_output.size())\n","  # print(local_labels.size())\n","  loss = lossfunction(local_output, local_labels)\n","\n","  ### backward pass\n","  loss.backward()\n","  torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n","  optimiser.step()\n","  epoch_loss += loss.item()\n","  return epoch_loss \n","\n","def evaluate(model, X_test,y_test, lossfunction):\n","  \n","  model.eval()\n","  epoch_loss = 0\n","  with torch.no_grad():\n","    local_batch, local_labels = X_test.unsqueeze(1).to(device), y_test.unsqueeze(1).to(device) \n","    local_labels = local_labels.unsqueeze(2)\n","          \n","    local_output = model(seq2seq_input=local_batch, target=local_labels, teacher_forcing_ratio =0)\n","\n","    local_output = local_output.view(-1)[1:]\n","    local_labels = local_labels.view(-1)[1:]\n","  \n","    loss = lossfunction(local_output, local_labels)\n","    epoch_loss += loss.item()\n","  return epoch_loss\n","\n","def running(grid,model,optimiser,lossfunction,X_train,y_train,X_test,y_test):\n","\n","  best_valid_loss = float('inf')\n","\n","  for epoch in range(grid['max_epochs']):\n","\n","    train_loss = training(model, X_train, y_train, optimiser, lossfunction, grid['clip'])\n","    valid_loss = evaluate(model, X_test,y_test, lossfunction)\n","\n","    if valid_loss < best_valid_loss:\n","      best_valid_loss = valid_loss\n","      print(f'Epoch: {epoch+1}:')\n","      print(f'Train Loss: {train_loss:.3f}')\n","      print(f'Validation Loss: {valid_loss:.3f}')\n","\n","  return best_valid_loss"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n","accu grad\n","\n","tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0.], device='cuda:0')\n","\n","weight of a layer\n","\n","Parameter containing:\n","tensor([[ 0.0535, -0.0186,  0.0476,  ...,  0.0609,  0.0702,  0.0300],\n","        [-0.0243, -0.0043,  0.0394,  ..., -0.0671, -0.0652, -0.0526],\n","        [-0.0673,  0.0480, -0.0358,  ...,  0.0633,  0.0214,  0.0235],\n","        ...,\n","        [-0.0438,  0.0349,  0.0179,  ...,  0.0641,  0.0388, -0.0305],\n","        [ 0.0624,  0.0196,  0.0396,  ..., -0.0652, -0.0617,  0.0620],\n","        [ 0.0142, -0.0122,  0.0099,  ...,  0.0294, -0.0241, -0.0059]],\n","       device='cuda:0', requires_grad=True)\n","\n","accu grad\n","\n","tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0.], device='cuda:0')\n","\n","weight of a layer\n","\n","Parameter containing:\n","tensor([[ 0.0535, -0.0186,  0.0476,  ...,  0.0609,  0.0702,  0.0300],\n","        [-0.0243, -0.0043,  0.0394,  ..., -0.0671, -0.0652, -0.0526],\n","        [-0.0673,  0.0480, -0.0358,  ...,  0.0633,  0.0214,  0.0235],\n","        ...,\n","        [-0.0438,  0.0349,  0.0179,  ...,  0.0641,  0.0388, -0.0305],\n","        [ 0.0624,  0.0196,  0.0396,  ..., -0.0652, -0.0617,  0.0620],\n","        [ 0.0142, -0.0122,  0.0099,  ...,  0.0294, -0.0241, -0.0059]],\n","       device='cuda:0', requires_grad=True)\n","\n","accu grad\n","\n","tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0.], device='cuda:0')\n","\n","weight of a layer\n","\n","Parameter containing:\n","tensor([[ 0.0535, -0.0186,  0.0476,  ...,  0.0609,  0.0702,  0.0300],\n","        [-0.0243, -0.0043,  0.0394,  ..., -0.0671, -0.0652, -0.0526],\n","        [-0.0673,  0.0480, -0.0358,  ...,  0.0633,  0.0214,  0.0235],\n","        ...,\n","        [-0.0438,  0.0349,  0.0179,  ...,  0.0641,  0.0388, -0.0305],\n","        [ 0.0624,  0.0196,  0.0396,  ..., -0.0652, -0.0617,  0.0620],\n","        [ 0.0142, -0.0122,  0.0099,  ...,  0.0294, -0.0241, -0.0059]],\n","       device='cuda:0', requires_grad=True)\n","\n","accu grad\n","\n","tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0.], device='cuda:0')\n","\n","weight of a layer\n","\n","Parameter containing:\n","tensor([[ 0.0535, -0.0186,  0.0476,  ...,  0.0609,  0.0702,  0.0300],\n","        [-0.0243, -0.0043,  0.0394,  ..., -0.0671, -0.0652, -0.0526],\n","        [-0.0673,  0.0480, -0.0358,  ...,  0.0633,  0.0214,  0.0235],\n","        ...,\n","        [-0.0438,  0.0349,  0.0179,  ...,  0.0641,  0.0388, -0.0305],\n","        [ 0.0624,  0.0196,  0.0396,  ..., -0.0652, -0.0617,  0.0620],\n","        [ 0.0142, -0.0122,  0.0099,  ...,  0.0294, -0.0241, -0.0059]],\n","       device='cuda:0', requires_grad=True)\n","\n","accu grad\n","\n","tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0.], device='cuda:0')\n","\n","weight of a layer\n","\n","Parameter containing:\n","tensor([[ 0.0535, -0.0186,  0.0476,  ...,  0.0609,  0.0702,  0.0300],\n","        [-0.0243, -0.0043,  0.0394,  ..., -0.0671, -0.0652, -0.0526],\n","        [-0.0673,  0.0480, -0.0358,  ...,  0.0633,  0.0214,  0.0235],\n","        ...,\n","        [-0.0438,  0.0349,  0.0179,  ...,  0.0641,  0.0388, -0.0305],\n","        [ 0.0624,  0.0196,  0.0396,  ..., -0.0652, -0.0617,  0.0620],\n","        [ 0.0142, -0.0122,  0.0099,  ...,  0.0294, -0.0241, -0.0059]],\n","       device='cuda:0', requires_grad=True)\n","\n","accu grad\n","\n","tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0.], device='cuda:0')\n","\n","weight of a layer\n","\n","Parameter containing:\n","tensor([[ 0.0535, -0.0186,  0.0476,  ...,  0.0609,  0.0702,  0.0300],\n","        [-0.0243, -0.0043,  0.0394,  ..., -0.0671, -0.0652, -0.0526],\n","        [-0.0673,  0.0480, -0.0358,  ...,  0.0633,  0.0214,  0.0235],\n","        ...,\n","        [-0.0438,  0.0349,  0.0179,  ...,  0.0641,  0.0388, -0.0305],\n","        [ 0.0624,  0.0196,  0.0396,  ..., -0.0652, -0.0617,  0.0620],\n","        [ 0.0142, -0.0122,  0.0099,  ...,  0.0294, -0.0241, -0.0059]],\n","       device='cuda:0', requires_grad=True)\n","\n","accu grad\n","\n","tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0.], device='cuda:0')\n","\n","weight of a layer\n","\n","Parameter containing:\n","tensor([[ 0.0535, -0.0186,  0.0476,  ...,  0.0609,  0.0702,  0.0300],\n","        [-0.0243, -0.0043,  0.0394,  ..., -0.0671, -0.0652, -0.0526],\n","        [-0.0673,  0.0480, -0.0358,  ...,  0.0633,  0.0214,  0.0235],\n","        ...,\n","        [-0.0438,  0.0349,  0.0179,  ...,  0.0641,  0.0388, -0.0305],\n","        [ 0.0624,  0.0196,  0.0396,  ..., -0.0652, -0.0617,  0.0620],\n","        [ 0.0142, -0.0122,  0.0099,  ...,  0.0294, -0.0241, -0.0059]],\n","       device='cuda:0', requires_grad=True)\n","\n","accu grad\n","\n","tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0.], device='cuda:0')\n","\n","weight of a layer\n","\n","Parameter containing:\n","tensor([[ 0.0535, -0.0186,  0.0476,  ...,  0.0609,  0.0702,  0.0300],\n","        [-0.0243, -0.0043,  0.0394,  ..., -0.0671, -0.0652, -0.0526],\n","        [-0.0673,  0.0480, -0.0358,  ...,  0.0633,  0.0214,  0.0235],\n","        ...,\n","        [-0.0438,  0.0349,  0.0179,  ...,  0.0641,  0.0388, -0.0305],\n","        [ 0.0624,  0.0196,  0.0396,  ..., -0.0652, -0.0617,  0.0620],\n","        [ 0.0142, -0.0122,  0.0099,  ...,  0.0294, -0.0241, -0.0059]],\n","       device='cuda:0', requires_grad=True)\n","\n","accu grad\n","\n","tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0.], device='cuda:0')\n","\n","weight of a layer\n","\n","Parameter containing:\n","tensor([[ 0.0535, -0.0186,  0.0476,  ...,  0.0609,  0.0702,  0.0300],\n","        [-0.0243, -0.0043,  0.0394,  ..., -0.0671, -0.0652, -0.0526],\n","        [-0.0673,  0.0480, -0.0358,  ...,  0.0633,  0.0214,  0.0235],\n","        ...,\n","        [-0.0438,  0.0349,  0.0179,  ...,  0.0641,  0.0388, -0.0305],\n","        [ 0.0624,  0.0196,  0.0396,  ..., -0.0652, -0.0617,  0.0620],\n","        [ 0.0142, -0.0122,  0.0099,  ...,  0.0294, -0.0241, -0.0059]],\n","       device='cuda:0', requires_grad=True)\n","\n","accu grad\n","\n","tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0.], device='cuda:0')\n","\n","weight of a layer\n","\n","Parameter containing:\n","tensor([[ 0.0535, -0.0186,  0.0476,  ...,  0.0609,  0.0702,  0.0300],\n","        [-0.0243, -0.0043,  0.0394,  ..., -0.0671, -0.0652, -0.0526],\n","        [-0.0673,  0.0480, -0.0358,  ...,  0.0633,  0.0214,  0.0235],\n","        ...,\n","        [-0.0438,  0.0349,  0.0179,  ...,  0.0641,  0.0388, -0.0305],\n","        [ 0.0624,  0.0196,  0.0396,  ..., -0.0652, -0.0617,  0.0620],\n","        [ 0.0142, -0.0122,  0.0099,  ...,  0.0294, -0.0241, -0.0059]],\n","       device='cuda:0', requires_grad=True)\n","\n","accu grad\n","\n","tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0.], device='cuda:0')\n","\n","weight of a layer\n","\n","Parameter containing:\n","tensor([[ 0.0535, -0.0186,  0.0476,  ...,  0.0609,  0.0702,  0.0300],\n","        [-0.0243, -0.0043,  0.0394,  ..., -0.0671, -0.0652, -0.0526],\n","        [-0.0673,  0.0480, -0.0358,  ...,  0.0633,  0.0214,  0.0235],\n","        ...,\n","        [-0.0438,  0.0349,  0.0179,  ...,  0.0641,  0.0388, -0.0305],\n","        [ 0.0624,  0.0196,  0.0396,  ..., -0.0652, -0.0617,  0.0620],\n","        [ 0.0142, -0.0122,  0.0099,  ...,  0.0294, -0.0241, -0.0059]],\n","       device='cuda:0', requires_grad=True)\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-54-6b5e754643a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-52-e9720a7ca4cf>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(grid, X_train, y_train, model)\u001b[0m\n\u001b[1;32m     39\u001b[0m       \u001b[0mtrainloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m       \u001b[0moptimiser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"KsDwbdu-t-OM","colab_type":"code","colab":{}},"source":["grid = {'max_epochs':512,\n","        'batch_size':-1, # full batch\n","        'learning_rate':1e-3,\n","        'clip':10\n","      }\n","\n","##### model ######\n","OUTPUT_DIM = 1\n","ENC_EMB_DIM = 21\n","#DEC_EMB_DIM = 1\n","ENC_HID_DIM = 32\n","DEC_HID_DIM = 32\n","ENC_DROPOUT = 0\n","DEC_DROPOUT = 0\n","\n","attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n","enc = Encoder(ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n","dec = Decoder(output_dim = OUTPUT_DIM,  enc_hid_dim = ENC_HID_DIM, dec_hid_dim = DEC_HID_DIM, dropout = DEC_DROPOUT,attention= attn)\n","\n","model = Seq2Seq(enc, dec, device).to(device)\n","optimiser = optim.Adam(model.parameters(),lr = grid['learning_rate'])\n","lossfunction = nn.MSELoss().to(device)\n","model.apply(utility.init_weights)\n","model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gJnUbUrwZpN6","colab_type":"code","colab":{}},"source":["running(grid,model,optimiser,lossfunction,X_train,y_train,X_test,y_test)"],"execution_count":0,"outputs":[]}]}